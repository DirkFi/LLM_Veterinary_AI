{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# !pip install langchain langchain-experimental langchain-chroma pillow open_clip_torch torch matplotlib unstructured pydantic\n",
    "import os\n",
    "from textbook_loading import (\n",
    "    load_book,\n",
    "    clean_and_categorize_elements,\n",
    "    semantic_chunk_texts,\n",
    "    enrich_image_context,\n",
    "    summarize_elements,\n",
    "    store_in_chromadb,\n",
    "    delete_irrelevant_images\n",
    ")\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = './data/MediumExample_Ears_17Pgs.pdf'\n",
    "image_output_dir = './figures/Ears_17Pgs'\n",
    "chroma_persist_dir = './chroma/Ears_17Pgs/'\n",
    "\n",
    "# Make sure the data directory exists\n",
    "assert os.path.exists('./data'), \"Error: './data' directory not found.\"\n",
    "assert os.path.exists(pdf_file), f\"Error: PDF file not found at {pdf_file}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìù Unstructuring textbooks, filtering junks, semanic chunking...\")\n",
    "raw_pdf_elements = load_book(pdf_file, image_output_dir)\n",
    "print(\"üéâ 1.process_pdf_with_semantic_chunking complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Clean and categorize\n",
    "texts, tables, images_raw, headers_raw, titles_raw, footers_raw, figure_captions_raw, list_items_raw = clean_and_categorize_elements(raw_pdf_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Semantic chunking\n",
    "embedding_model = OpenCLIPEmbeddings(model_name=\"ViT-L-14\", checkpoint=\"laion2b_s32b_b82k\")\n",
    "semantic_chunks = semantic_chunk_texts(texts, embedding_model, n_clusters=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Enrich image context (hybrid)\n",
    "enrich_image_context(images_raw, raw_pdf_elements, embedding_model=embedding_model, semantic_chunks=semantic_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Summarize, store, etc.\n",
    "text_summaries, table_summaries, image_paths, relevant_images_to_summarize, image_summaries = summarize_elements(\n",
    "    texts, tables, images_raw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = store_in_chromadb(\n",
    "    text_summaries, texts, table_summaries, tables, image_paths,\n",
    "    relevant_images_to_summarize, image_summaries,\n",
    "    persist_directory=chroma_persist_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_irrelevant_images(images_raw, relevant_images_to_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "sound_file = \"/System/Library/Sounds/Glass.aiff\"\n",
    "os.system(f\"afplay '{sound_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting Retrieved Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"tell me about the anatomy of the ear?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print('-'*40, \"Here are the retrieved original_docs with similarity scores\",'-'*40)\n",
    "\n",
    "results = retriever.retrieve_multi_modal(query, k=10)\n",
    "for res in results:\n",
    "    if res[\"modality\"] == \"image\" and os.path.exists(res[\"summary\"]):\n",
    "        display(Image(filename=res[\"summary\"]))\n",
    "    print('-' * 40)\n",
    "    print(f\"üîé Similarity Score: {res['score']:.4f}\")\n",
    "    print(f\"üñºÔ∏è Modality: {res['modality']}\")\n",
    "    print(\"üìù Summary/Chunk:\", res[\"summary\"])\n",
    "    print(\"Summary Metadata:\", res[\"original_metadata\"])\n",
    "    print(\"Doc ID:\", res[\"doc_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only images\n",
    "image_results = retriever.retrieve(query, k=10, filter={\"type\": \"image\"})\n",
    "# Only text\n",
    "text_results = retriever.retrieve(query, k=10, filter={\"type\": \"text\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
