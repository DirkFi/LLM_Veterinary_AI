{
 "cells": [
  {
   "cell_type": "code",
   "source": "def test_complete_workflow(query, image_path=None):\n    \"\"\"\n    Test the complete veterinary AI workflow with a given query.\n    \"\"\"\n    print(f\"üîç Testing Query: {query}\")\n    if image_path:\n        print(f\"üì∏ Image: {image_path}\")\n    print(\"=\"*60)\n    \n    # Initialize state\n    initial_state = {\n        \"text_query\": query,\n        \"image_path\": image_path,\n        \"loop_count\": 0,\n        \"path_taken\": []\n    }\n    \n    try:\n        # Run the complete workflow\n        result = vet_graph.invoke(initial_state)\n        \n        # Display results\n        print(\"üéØ Final Result:\")\n        print(result.get(\"final_answer\", \"No answer generated\"))\n        print(\"\\\\n\" + \"=\"*60)\n        \n        # Show path taken\n        path_taken = result.get(\"path_taken\", [])\n        if path_taken:\n            print(f\"üõ§Ô∏è  Path taken: {' -> '.join(path_taken)}\")\n        \n        # Show query classification\n        query_type = result.get(\"query_type\", \"Unknown\")\n        print(f\"üìÇ Query classified as: {query_type}\")\n        \n        # Show hallucination check results if applicable\n        if \"hallucination_check\" in result:\n            hallucination_result = result[\"hallucination_check\"]\n            print(f\"‚úÖ Hallucination check: {'PASSED' if hallucination_result else 'FAILED'}\")\n        \n        return result\n        \n    except Exception as e:\n        print(f\"‚ùå Error in workflow: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n# Test cases\nprint(\"Testing different types of queries:\\\\n\")\n\n# Test 1: Regular Q&A query\ntest_complete_workflow(\"My cat has been scratching its ear a lot and I see dark stuff inside. What could this be?\")\n\nprint(\"\\\\n\" + \"=\"*80 + \"\\\\n\")\n\n# Test 2: Emergency query\ntest_complete_workflow(\"My cat is bleeding heavily from a deep wound and seems unconscious!\")\n\nprint(\"\\\\n\" + \"=\"*80 + \"\\\\n\")\n\n# Test 3: Irrelevant query\ntest_complete_workflow(\"How do I fix my car's engine?\")\n\nprint(\"\\\\n\" + \"=\"*80 + \"\\\\n\")\n\n# Test 4: Query with image\ntest_complete_workflow(\"What's wrong with my cat's ear?\", image_path=\"../cat_ear_problem.jpeg\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Test Complete Workflow",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from langgraph.graph import StateGraph, END\n\ndef create_veterinary_graph():\n    \"\"\"\n    Create the complete LangGraph workflow for the veterinary AI assistant.\n    \"\"\"\n    \n    # Create the graph\n    workflow = StateGraph(GraphState)\n    \n    # Add nodes\n    workflow.add_node(\"query_handler\", query_handler)\n    workflow.add_node(\"query_refinement\", query_refinement_node)\n    workflow.add_node(\"query_decomposition\", query_decomposition)\n    workflow.add_node(\"contextual_retrieval\", contextual_retrieval_flat)\n    workflow.add_node(\"rerank\", rerank_node_hybrid_v2)\n    workflow.add_node(\"thinking\", thinking_node)\n    workflow.add_node(\"answer_generation\", answer_generation_node)\n    workflow.add_node(\"hallucination_check\", hallucination_check_node)\n    workflow.add_node(\"emergency_handler\", emergency_handler_node)\n    workflow.add_node(\"irrelevant_handler\", irrelevant_query_handler)\n    \n    # Set entry point\n    workflow.set_entry_point(\"query_handler\")\n    \n    # Define conditional routing after query_handler\n    def route_after_query_handler(state):\n        query_type = state.get(\"query_type\", \"\")\n        if query_type == \"emergency\":\n            return \"emergency_handler\"\n        elif query_type == \"q&a\":\n            return \"query_refinement\"\n        else:  # irrelevant\n            return \"irrelevant_handler\"\n    \n    # Add conditional routing\n    workflow.add_conditional_edges(\n        \"query_handler\",\n        route_after_query_handler,\n        {\n            \"emergency_handler\": \"emergency_handler\",\n            \"query_refinement\": \"query_refinement\", \n            \"irrelevant_handler\": \"irrelevant_handler\"\n        }\n    )\n    \n    # Q&A path edges\n    workflow.add_edge(\"query_refinement\", \"query_decomposition\")\n    workflow.add_edge(\"query_decomposition\", \"contextual_retrieval\")\n    workflow.add_edge(\"contextual_retrieval\", \"rerank\")\n    workflow.add_edge(\"rerank\", \"thinking\")\n    workflow.add_edge(\"thinking\", \"answer_generation\")\n    workflow.add_edge(\"answer_generation\", \"hallucination_check\")\n    \n    # Conditional routing after hallucination check\n    def route_after_hallucination_check(state):\n        hallucination_check = state.get(\"hallucination_check\", False)\n        if hallucination_check:\n            return \"finalize_answer\"\n        else:\n            return \"regenerate_answer\"\n    \n    def finalize_answer(state):\n        \"\"\"Finalize the answer after hallucination check passes.\"\"\"\n        return {\n            \"final_answer\": state.get(\"generated_answer\", \"\"),\n            \"path_taken\": state.get(\"path_taken\", []) + [\"Q&A_path_completed\"]\n        }\n    \n    def regenerate_answer(state):\n        \"\"\"Regenerate answer if hallucination check fails.\"\"\"\n        # For now, we'll use the original answer but add a warning\n        original_answer = state.get(\"generated_answer\", \"\")\n        warning = \"\\\\n\\\\n‚ö†Ô∏è Please note: Some information may need verification. Always consult with a veterinarian for accurate diagnosis and treatment.\"\n        \n        return {\n            \"final_answer\": original_answer + warning,\n            \"path_taken\": state.get(\"path_taken\", []) + [\"regenerated_with_warning\"]\n        }\n    \n    # Add the finalize and regenerate nodes\n    workflow.add_node(\"finalize_answer\", finalize_answer)\n    workflow.add_node(\"regenerate_answer\", regenerate_answer)\n    \n    # Add conditional edges for hallucination check\n    workflow.add_conditional_edges(\n        \"hallucination_check\",\n        route_after_hallucination_check,\n        {\n            \"finalize_answer\": \"finalize_answer\",\n            \"regenerate_answer\": \"regenerate_answer\"\n        }\n    )\n    \n    # Terminal edges\n    workflow.add_edge(\"emergency_handler\", END)\n    workflow.add_edge(\"irrelevant_handler\", END)\n    workflow.add_edge(\"finalize_answer\", END)\n    workflow.add_edge(\"regenerate_answer\", END)\n    \n    # Compile the graph\n    app = workflow.compile()\n    return app\n\n# Create the graph\nvet_graph = create_veterinary_graph()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Complete LangGraph Workflow",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def irrelevant_query_handler(state):\n    \"\"\"\n    Handle queries that are not related to veterinary topics.\n    \"\"\"\n    text_query = state.get(\"text_query\", \"\")\n    \n    response_message = f\"\"\"\n    I'm a veterinary AI assistant designed to help with animal health and pet care questions. \n    \n    Your query: \"{text_query}\"\n    \n    This doesn't appear to be related to veterinary medicine, animal health, or pet care. \n    \n    I can help you with:\n    ‚Ä¢ Animal health symptoms and concerns\n    ‚Ä¢ Pet care advice\n    ‚Ä¢ Veterinary procedures and treatments\n    ‚Ä¢ Emergency animal care\n    ‚Ä¢ General pet wellness questions\n    \n    Please feel free to ask me anything related to animal health or pet care!\n    \"\"\"\n    \n    return {\n        \"final_answer\": response_message\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def emergency_handler_node(state):\n    \"\"\"\n    Handle emergency veterinary situations with urgent instructions.\n    \"\"\"\n    text_query = state.get(\"text_query\", \"\")\n    image_path = state.get(\"image_path\", None)\n    \n    # Get image summary if provided\n    image_summary = \"\"\n    if image_path and os.path.exists(image_path):\n        image_summary = get_image_summary(image_path)\n    \n    # Quick retrieval for emergency information\n    retriever = init_retriever()\n    emergency_terms = [\"emergency\", \"urgent\", \"bleeding\", \"unconscious\", \"breathing\", \"poison\", \"trauma\"]\n    \n    # Search for emergency-related information\n    emergency_results = []\n    for term in emergency_terms:\n        results = retriever.retrieve_multi_modal(f\"emergency {term} first aid\", k=3)\n        emergency_results.extend(results)\n    \n    # Remove duplicates and get top results\n    seen_ids = set()\n    unique_emergency_results = []\n    for result in emergency_results:\n        doc_id = result.get(\"doc_id\")\n        if doc_id and doc_id not in seen_ids:\n            seen_ids.add(doc_id)\n            unique_emergency_results.append(result)\n    \n    # Prepare emergency context\n    emergency_context = \"\"\n    for doc in unique_emergency_results[:5]:  # Top 5 emergency docs\n        modality = doc.get(\"modality\") or (doc.get(\"original_metadata\") or {}).get(\"type\")\n        if modality == \"text\":\n            emergency_context += f\"[EMERGENCY INFO] {doc.get('summary', '')}\\n\"\n    \n    emergency_prompt = f\"\"\"\n    üö® VETERINARY EMERGENCY RESPONSE üö®\n    \n    You are responding to a potential veterinary emergency. This requires immediate, clear, and actionable guidance.\n    \n    User's Emergency: {text_query}\n    {f\"Visual Information: {image_summary}\" if image_summary else \"\"}\n    \n    Emergency Information from Database:\n    {emergency_context}\n    \n    CRITICAL INSTRUCTIONS:\n    1. START with \"‚ö†Ô∏è EMERGENCY: Contact your veterinarian or emergency animal hospital IMMEDIATELY\"\n    2. Provide immediate first aid steps if applicable\n    3. List warning signs that require URGENT attention\n    4. Give clear, step-by-step instructions\n    5. End with emergency contact reminders\n    \n    Provide immediate, life-saving guidance while emphasizing professional veterinary care.\n    \"\"\"\n    \n    messages = [{\n        \"role\": \"user\",\n        \"content\": emergency_prompt\n    }]\n    \n    response = ollama.chat(\n        model=\"llama3.2:3b\",\n        messages=messages,\n        options={\"temperature\": 0.2}  # Low temperature for consistency in emergencies\n    )\n    \n    emergency_instructions = response['message']['content']\n    \n    return {\n        \"emergency_instructions\": emergency_instructions,\n        \"emergency_retrieved_docs\": unique_emergency_results[:5]\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def hallucination_check_node(state):\n    \"\"\"\n    Check if the generated answer contains hallucinations or unsupported claims.\n    \"\"\"\n    generated_answer = state.get(\"generated_answer\", \"\")\n    context_for_answer = state.get(\"context_for_answer\", \"\")\n    \n    hallucination_prompt = f\"\"\"\n    You are a fact-checker for veterinary information. Your task is to verify if the generated answer \n    is supported by the provided context and doesn't contain hallucinations or unsupported claims.\n    \n    Generated Answer:\n    {generated_answer}\n    \n    Supporting Context:\n    {context_for_answer}\n    \n    Please evaluate:\n    1. Are all claims in the answer supported by the context?\n    2. Are there any factual inaccuracies?\n    3. Are there any overly specific medical recommendations that go beyond the context?\n    4. Does the answer maintain appropriate disclaimers about veterinary consultation?\n    \n    Respond with:\n    - \"PASS\" if the answer is well-supported and appropriate\n    - \"FAIL\" if there are significant hallucinations or unsupported claims\n    - Include a brief explanation of your assessment\n    \"\"\"\n    \n    messages = [{\n        \"role\": \"user\",\n        \"content\": hallucination_prompt\n    }]\n    \n    response = ollama.chat(\n        model=\"llama3.2:3b\",\n        messages=messages,\n        options={\"temperature\": 0.1}  # Low temperature for consistency\n    )\n    \n    check_result = response['message']['content']\n    hallucination_check = \"PASS\" in check_result.upper()\n    \n    return {\n        \"hallucination_check\": hallucination_check,\n        \"hallucination_details\": check_result\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def answer_generation_node(state):\n    \"\"\"\n    Generate a comprehensive answer based on the thinking analysis and context.\n    \"\"\"\n    text_query = state.get(\"text_query\", \"\")\n    image_path = state.get(\"image_path\", None)\n    thinking_analysis = state.get(\"thinking_analysis\", \"\")\n    context_for_answer = state.get(\"context_for_answer\", \"\")\n    reranked_docs = state.get(\"reranked_docs\", [])\n    \n    # Get image summary if image provided\n    image_summary = \"\"\n    if image_path and os.path.exists(image_path):\n        image_summary = get_image_summary(image_path)\n    \n    # Prepare image references from top docs\n    image_references = []\n    for doc in reranked_docs[:5]:  # Top 5 docs\n        modality = doc.get(\"modality\") or (doc.get(\"original_metadata\") or {}).get(\"type\")\n        if modality in [\"image\", \"image_summary\"]:\n            img_path = (doc.get(\"original_metadata\") or {}).get(\"image_path\")\n            if img_path:\n                image_references.append(f\"Reference image: {img_path}\")\n    \n    answer_prompt = f\"\"\"\n    You are a knowledgeable veterinary assistant providing helpful information to pet owners.\n    \n    IMPORTANT GUIDELINES:\n    1. Always emphasize consulting with a veterinarian for proper diagnosis and treatment\n    2. Provide factual, evidence-based information\n    3. Include relevant warnings about emergency situations\n    4. Be empathetic and supportive in tone\n    5. Reference specific information from the retrieved documents\n    6. If images are mentioned, describe what they show\n    \n    User Query: {text_query}\n    \n    {f\"User's Image Description: {image_summary}\" if image_summary else \"\"}\n    \n    Analysis from Thinking Process:\n    {thinking_analysis}\n    \n    Retrieved Information:\n    {context_for_answer}\n    \n    {f\"Relevant Reference Images Available: {chr(10).join(image_references)}\" if image_references else \"\"}\n    \n    Please provide a comprehensive, helpful response that addresses the user's concern while following the guidelines above.\n    \"\"\"\n    \n    messages = [{\n        \"role\": \"user\",\n        \"content\": answer_prompt\n    }]\n    \n    response = ollama.chat(\n        model=\"llama3.2:3b\",\n        messages=messages,\n        options={\"temperature\": 0.4}\n    )\n    \n    generated_answer = response['message']['content']\n    \n    return {\n        \"generated_answer\": generated_answer\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def thinking_node(state):\n    \"\"\"\n    Thinking node analyzes the user's intent and the reranked documents\n    to determine what information is needed for a comprehensive answer.\n    \"\"\"\n    text_query = state.get(\"text_query\", \"\")\n    refined_query = state.get(\"refined_query\", \"\")\n    reranked_docs = state.get(\"reranked_docs\", [])\n    \n    # Extract top docs for analysis\n    top_docs = reranked_docs[:10]  # Use top 10 docs\n    \n    # Prepare context from top documents\n    context_pieces = []\n    for doc in top_docs:\n        modality = doc.get(\"modality\") or (doc.get(\"original_metadata\") or {}).get(\"type\")\n        if modality == \"text\":\n            context_pieces.append(f\"[TEXT] {doc.get('summary', '')}\")\n        elif modality in [\"image\", \"image_summary\"]:\n            context_pieces.append(f\"[IMAGE] {doc.get('summary', '')}\")\n    \n    context = \"\\n\".join(context_pieces)\n    \n    thinking_prompt = f\"\"\"\n    You are a veterinary AI assistant analyzing a user query and retrieved information.\n    Your task is to think through what the user needs and how to structure a comprehensive answer.\n    \n    Original Query: {text_query}\n    Refined Query: {refined_query}\n    \n    Retrieved Information:\n    {context}\n    \n    Please analyze:\n    1. What is the user's main concern or question?\n    2. What key information from the retrieved docs addresses their concern?\n    3. What additional context or warnings should be included?\n    4. Are there any gaps in the information that need to be acknowledged?\n    \n    Provide a structured analysis that will guide the answer generation.\n    \"\"\"\n    \n    messages = [{\n        \"role\": \"user\",\n        \"content\": thinking_prompt\n    }]\n    \n    response = ollama.chat(\n        model=\"llama3.2:3b\",\n        messages=messages,\n        options={\"temperature\": 0.3}\n    )\n    \n    analysis = response['message']['content']\n    \n    return {\n        \"thinking_analysis\": analysis,\n        \"context_for_answer\": context\n    }",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LangSmith API\n",
    "Retrievals can be traced here for easier debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGSMITH_API_KEY'] = '123-123-213-123-123-123'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LangGraph Flow](../../langgraph%20designs/graph_design_v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    # Core user input\n",
    "    text_query: str\n",
    "    image_path: Optional[str]  # Path to uploaded image, if any\n",
    "\n",
    "    # Routing/intent\n",
    "    query_type : str # currently being divide into 'emergency'/'Q&A'/'irrelevant'.\n",
    "    \n",
    "    # Q&A path\n",
    "    refined_query: Optional[str]\n",
    "    sub_queries: Optional[List[str]]\n",
    "    current_sub_query: Optional[str]\n",
    "    retrieved_docs: Optional[List[Dict[str, Any]]]  # Results from retrieval\n",
    "    reranked_docs: Optional[List[Dict[str, Any]]]   # After rerank step\n",
    "\n",
    "    # Feedback loop\n",
    "    followup_questions: Optional[List[str]]\n",
    "    user_responses: Optional[List[str]]\n",
    "    loop_count: int\n",
    "\n",
    "    # Answer generation\n",
    "    generated_answer: Optional[str]\n",
    "    hallucination_check: Optional[bool]\n",
    "    answer_sufficient: Optional[bool]\n",
    "\n",
    "    # Emergency path\n",
    "    emergency_instructions: Optional[str]\n",
    "    emergency_retrieved_docs: Optional[List[Dict[str, Any]]]\n",
    "\n",
    "    # Web search\n",
    "    web_search_results: Optional[List[Dict[str, Any]]]\n",
    "\n",
    "    # Final output\n",
    "    final_answer: Optional[str]\n",
    "\n",
    "    # Misc/trace/debug\n",
    "    path_taken: Optional[List[str]]\n",
    "    error: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Graph Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Handler Node \n",
    "\n",
    "Before LLM analyze user query and image, it will be assessed with \"Is this veterinary-related?\". This will ensure our AI tool will not be used for other purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_handler(state):\n",
    "    text_query = state.get(\"text_query\", \"\")\n",
    "    image_path = state.get(\"image_path\", None)\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a domain classifier for a veterinary assistant. \"\n",
    "        \"If an image is provided, understand the image from veterinary point of view.\"\n",
    "        \"A user query is the combination of text query and image(if there is). \"\n",
    "        \"Then, classify the user query into one of three categories:\\n\"\n",
    "        \"1. 'emergency' ‚Äî If the user query is about a veterinary emergency (e.g., mass bleeding, serious bone fracture, unconsciousness, severe breathing difficulty, or other life-threatening situations).\\n\"\n",
    "        \"2. 'Q&A' ‚Äî If the user query is about is about general veterinary questions, symptom checks, or non-emergency animal health issues.\\n\\n\"\n",
    "        \"3. 'irrelevant' ‚Äî If the user query is NOT about veterinary, animal health, pet care, etc.\\n\"\n",
    "        \"Your response must be exactly one of: 'irrelevant', 'emergency', or 'Q&A'. Do not explain your answer or add anything else.\\n\\n\"\n",
    "        f\"User input: {text_query}\\n\"\n",
    "    )\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "        \"images\": []\n",
    "    }]\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        messages[0][\"images\"].append(image_path)\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"minicpm-v:8b\",\n",
    "        messages=messages,\n",
    "        options={\"temperature\": 0.2}\n",
    "    )\n",
    "    result = response['message']['content'].strip().lower()\n",
    "    # Only allow the three valid outputs\n",
    "    if result not in ['irrelevant', 'emergency', 'q&a']:\n",
    "        result = 'irrelevant'\n",
    "    \n",
    "    return {\"query_type\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Query: What vaccines does my cat need?\n",
      "Updated State: {'text_query': 'What vaccines does my cat need?', 'image_path': None, 'query_type': 'q&a'}\n",
      "Query Type: q&a\n",
      "----------------------------------------\n",
      "Input Query: My cat is bleeding a lot after being hit by a car.\n",
      "Updated State: {'text_query': 'My cat is bleeding a lot after being hit by a car.', 'image_path': None, 'query_type': 'emergency'}\n",
      "Query Type: emergency\n",
      "----------------------------------------\n",
      "Input Query: How do I fix my car engine?\n",
      "Updated State: {'text_query': 'How do I fix my car engine?', 'image_path': None, 'query_type': 'irrelevant'}\n",
      "Query Type: irrelevant\n",
      "----------------------------------------\n",
      "Input Query: What should I do?\n",
      "Image Path: ../emergency_cat.jpg\n",
      "Updated State: {'text_query': 'What should I do?', 'image_path': '../emergency_cat.jpg', 'query_type': 'emergency'}\n",
      "Query Type: emergency\n",
      "----------------------------------------\n",
      "Input Query: What should I feed to this cat?\n",
      "Image Path: ../skinny_cat.jpg\n",
      "Updated State: {'text_query': 'What should I feed to this cat?', 'image_path': '../skinny_cat.jpg', 'query_type': 'q&a'}\n",
      "Query Type: q&a\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_query_handler_node(query_handler, test_query, image_path=None):\n",
    "    # Build the initial state\n",
    "    state = {\n",
    "        \"text_query\": test_query,\n",
    "        \"image_path\": image_path\n",
    "    }\n",
    "    # Call the query handler node\n",
    "    new_state = query_handler(state)\n",
    "    # Print the results\n",
    "    print(\"Input Query:\", test_query)\n",
    "    if image_path:\n",
    "        print(\"Image Path:\", image_path)\n",
    "    print(\"Updated State:\", new_state)\n",
    "    print(\"Query Type:\", new_state.get(\"query_type\", \"N/A\"))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# --- Example usage ---\n",
    "test_query_handler_node(query_handler, \"What vaccines does my cat need?\")\n",
    "test_query_handler_node(query_handler, \"My cat is bleeding a lot after being hit by a car.\")\n",
    "test_query_handler_node(query_handler, \"How do I fix my car engine?\")\n",
    "test_query_handler_node(query_handler, \"What should I do?\", image_path=\"../emergency_cat.jpg\")\n",
    "test_query_handler_node(query_handler, \"What should I feed to this cat?\", image_path=\"../skinny_cat.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_summary(image_path):\n",
    "    prompt = \"\"\"From a feline veterinary stand point, provide a highly detailed and objective \n",
    "                description of the image. Focus on all observable elements, actions, \n",
    "                objects, subjects, their attributes (e.g., color, size, texture), \n",
    "                their spatial relationships, and any discernible context or implied scene. \n",
    "                Also focus on all possible health issue.\n",
    "                Describe any text present in the image. This description must be exhaustive \n",
    "                and purely factual, capturing every significant visual detail to serve as a \n",
    "                comprehensive textual representation for further analysis by another AI model. \n",
    "                If the image is entirely irrelevant or contains no discernible subject, \n",
    "                state \"No relevant visual information.\"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "        \"images\": [image_path]\n",
    "    }]\n",
    "    response = ollama.chat(\n",
    "        model=\"minicpm-v:8b\",\n",
    "        messages=messages,\n",
    "        options={\"temperature\": 0.2}\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "def query_refinement_node(state):\n",
    "    text_query = state.get(\"text_query\", \"\")\n",
    "    image_path = state.get(\"image_path\", None)\n",
    "    image_summary = get_image_summary(image_path) if image_path else \"\"\n",
    "\n",
    "    if image_summary:\n",
    "        prompt = (\n",
    "            \"You are a veterinary assistant AI. Your task is to rewrite and expand the user's query for a veterinary knowledge base search. \"\n",
    "            \"You are NOT being asked to provide medical advice, diagnosis, or treatment recommendations. \"\n",
    "            \"Your job is to help formulate a search query that could retrieve relevant veterinary information for a veterinarian or pet owner. \"\n",
    "            \"Use the image description to add context, but avoid making assumptions about the specific diagnosis or underlying causes unless explicitly stated. \"\n",
    "            \"Frame the refined query in an open-ended, unbiased way, considering a broad range of possible causes, diagnostic steps, and management options. \"\n",
    "            \"If the user describes symptoms, include them factually. \"\n",
    "            \"Do not presume the animal's overall health status or limit the query to only the most common conditions. \"\n",
    "            \"Output ONLY one single, context-rich, and unbiased query as a paragraph, and nothing else.\\n\\n\"\n",
    "            f\"User query: {text_query}\\n\"\n",
    "            f\"Image description: {image_summary}\\n\"\n",
    "            \"Refined query:\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"You are a veterinary assistant AI. Your task is to rewrite and expand the user's query for a veterinary knowledge base search. \"\n",
    "            f\"Consider add questions about possible causes, diagnostic considerations, anything that would be helpful in the situation, but combine everything into a single, comprehensive question or query. \"\n",
    "            f\"Output ONLY one single, context-rich query as a paragraph, and nothing else.\\n\\n\"\n",
    "            f\"User query: {text_query}\\n\"\n",
    "            f\"Refined query:\"\n",
    "        )\n",
    "\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }]\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2:3b\",  # or another strong text model\n",
    "        messages=messages,\n",
    "        options={\"temperature\": 0.3}\n",
    "    )\n",
    "    return {\"refined_query\": response['message']['content']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Query: What happened to my cat ear? It's being it for a long time. Sometimes I even see blood and wounds in its ear. \n",
      "Image Path: ../cat_ear_problem.jpeg\n",
      "Refined Query: What are possible causes and diagnostic steps for chronic ear infections or otitis in cats, characterized by visible brownish-orange debris, discharge, and wounds in the ear canal, potentially accompanied by signs of infection such as redness, swelling, and bleeding, and how can these conditions be distinguished from other potential health issues that may affect a cat's auditory system?\n",
      "----------------------------------------\n",
      "Input Query: My cat has being scratching its ear too often. There are some dark greasy thing in it. It sratch its ear so often and so hard that I see wounds and blood in it. What should I do?\n",
      "Refined Query: My cat is exhibiting excessive ear scratching, resulting in visible wounds and bleeding due to the presence of dark, greasy debris, which may indicate a skin infection or allergies; what are the possible underlying causes of this behavior, how can I diagnose the condition (e.g. are there any specific tests or examinations required), and what treatment options are available, including potential medications, dietary changes, or other interventions that could help alleviate my cat's discomfort and promote healing?\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_query_refinement_node(query_refinement_node, test_query, image_path=None):\n",
    "    # Build the initial state\n",
    "    state = {\n",
    "        \"text_query\": test_query,\n",
    "        \"image_path\": image_path\n",
    "    }\n",
    "    # Call the query refinement node\n",
    "    new_state = query_refinement_node(state)\n",
    "    # Print the results\n",
    "    print(\"Input Query:\", test_query)\n",
    "    if image_path:\n",
    "        print(\"Image Path:\", image_path)\n",
    "    print(\"Refined Query:\", new_state.get(\"refined_query\", \"N/A\"))\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# --- Example usage ---\n",
    "test_query_refinement_node(query_refinement_node, \"What happened to my cat ear? It's being it for a long time. Sometimes I even see blood and wounds in its ear. \", image_path=\"../cat_ear_problem.jpeg\")\n",
    "test_query_refinement_node(query_refinement_node, \"My cat has being scratching its ear too often. There are some dark greasy thing in it. It sratch its ear so often and so hard that I see wounds and blood in it. What should I do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "def query_decomposition(state):\n",
    "    refined_query = state['refined_query']\n",
    "\n",
    "    query_decomposition_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an intelligent assistant. Your task is to break down the given complex query\n",
    "    into a list of simpler, focused sub-queries. Each sub-query should be a standalone question\n",
    "    that can be used to retrieve specific information from a veterinary knowledge base.\n",
    "\n",
    "    At the end of your list, add 2-3 additional sub-queries that specifically require visual information or images.\n",
    "    For example, you might add:\n",
    "    - \"Show me an image of how to pick up a cat.\"\n",
    "    - \"Show me an image of how to do CPR for a cat.\"\n",
    "    - \"Show me a diagram of feline anatomy.\"\n",
    "    Be creative and make sure these visual sub-queries are relevant to the original complex query.\n",
    "\n",
    "    Output ONLY a valid JSON array of strings, and nothing else. Do not include any explanations, markdown, or extra text.\n",
    "\n",
    "    Complex query: {refined_query}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Create the query decomposition chain\n",
    "    query_decomposition_chain = (\n",
    "        query_decomposition_prompt  \n",
    "        | ChatOllama(model=\"llama3.2:3b\")  \n",
    "        | JsonOutputParser() \n",
    "    )\n",
    "\n",
    "    # --- Demonstration of query decomposition ---\n",
    "\n",
    "    print(f\"Original refined query: {refined_query[:300]} ....\")\n",
    "\n",
    "    decomposed_queries = query_decomposition_chain.invoke({\"refined_query\": refined_query})\n",
    "    # Try to extract the JSON array from the response\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    # print(f\"Decomposed queries:\\n{decomposed_queries}\")\n",
    "\n",
    "    print(f\"There are {len(decomposed_queries)} queries after decomposition \\n\")\n",
    "    print(f\"Here's a example of the first one: {decomposed_queries[0]}\")\n",
    "\n",
    "    return {\"sub_queries\": decomposed_queries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original refined query: What are possible causes and diagnostic steps for chronic ear infections or otitis in cats, characterized by visible brownish-orange debris, discharge, and wounds in the ear canal, potentially accompanied by signs of infection such as redness, swelling, and bleeding, and how can these conditions be  ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "There are 8 queries after decomposition \n",
      "\n",
      "Here's a example of the first one: What are common causes of chronic ear infections or otitis in cats?\n",
      "Decomposed Sub-Queries:\n",
      "['What are common causes of chronic ear infections or otitis in cats?', 'Describe diagnostic steps for detecting chronic ear infections or otitis in cats', 'What is the significance of visible brownish-orange debris, discharge, and wounds in the ear canal in cats?', 'How can redness, swelling, and bleeding be distinguished from other feline health issues affecting the auditory system?', 'What are potential complications if left untreated?', 'Show me an image of a cat with signs of otitis externa', \"Show me an image of how to properly clean a cat's ear canal\", 'Describe the anatomy of the feline ear canal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "def test_query_decomposition(query_decomposition_func, refined_query):\n",
    "    # Build the initial state\n",
    "    state = {\n",
    "        \"refined_query\": refined_query\n",
    "    }\n",
    "    # Call the query decomposition function\n",
    "    new_state = query_decomposition_func(state)\n",
    "    # Print the results\n",
    "    print(\"Decomposed Sub-Queries:\")\n",
    "    print(new_state['sub_queries'])\n",
    "\n",
    "# --- Example usage ---\n",
    "test_query_decomposition(\n",
    "    query_decomposition,\n",
    "    \"What are possible causes and diagnostic steps for chronic ear infections or otitis in cats, characterized by visible brownish-orange debris, discharge, and wounds in the ear canal, potentially accompanied by signs of infection such as redness, swelling, and bleeding, and how can these conditions be distinguished from other potential health issues that may affect a cat's auditory system?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Retrievals\n",
    "\n",
    "Based on decomposed sub queries, we are able to retrieve contexutally close aligned Documents from the vector database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Unified Retriever (Retrieve text, table, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def init_retriever():\n",
    "\n",
    "    # Instantiate the retriever\n",
    "    class UnifiedRetriever:\n",
    "        \"\"\"\n",
    "        UnifiedRetriever supports multi-modal retrieval from the vectorstore and docstore.\n",
    "        It can retrieve by text, image, or both (multi-query), and supports metadata filtering by modality.\n",
    "        \"\"\"\n",
    "        def __init__(self, vectorstore, docstore, id_key=\"doc_id\"):\n",
    "            self.vectorstore = vectorstore\n",
    "            self.docstore = docstore\n",
    "            self.id_key = id_key\n",
    "            self._collection = docstore._collection\n",
    "\n",
    "        def retrieve(self, query, k=5, filter=None):\n",
    "            \"\"\"\n",
    "            Retrieve top-k results for a query, optionally filtered by metadata (e.g., modality).\n",
    "            \"\"\"\n",
    "            results = self.vectorstore.similarity_search_with_score(query, k=k, filter=filter)\n",
    "            output = []\n",
    "            for doc, score in results:\n",
    "                doc_id = doc.metadata.get(self.id_key)\n",
    "                try:\n",
    "                    original = self._collection.get(ids=[doc_id], include=[\"documents\", \"metadatas\"])\n",
    "                    original_doc = original[\"documents\"][0] if original[\"documents\"] else None\n",
    "                    original_meta = original[\"metadatas\"][0] if original[\"metadatas\"] else None\n",
    "                except Exception as e:\n",
    "                    original_doc = None\n",
    "                    original_meta = None\n",
    "                output.append({\n",
    "                    \"summary\": doc.page_content,\n",
    "                    \"original\": original_doc,\n",
    "                    \"original_metadata\": original_meta,\n",
    "                    \"summary_metadata\": doc.metadata,\n",
    "                    \"score\": score\n",
    "                })\n",
    "            return output\n",
    "\n",
    "        def retrieve_multi_modal(self, query, k=5, text_types=(\"text\",), image_types=(\"image\", \"image_summary\")):\n",
    "            \"\"\"\n",
    "            Multi-Query/Multi-Modal Retrieval:\n",
    "            - Retrieves top-k text and top-k image/image_summary results for the query.\n",
    "            - Merges and sorts by score.\n",
    "            - Returns a list of results with modality info.\n",
    "            \"\"\"\n",
    "            # Retrieve text results\n",
    "            text_results = self.vectorstore.similarity_search_with_score(query, k=k, filter={\"type\": {\"$in\": list(text_types)}})\n",
    "            # Retrieve image/image_summary results\n",
    "            image_results = self.vectorstore.similarity_search_with_score(query, k=k, filter={\"type\": {\"$in\": list(image_types)}})\n",
    "            # Merge and sort by score (lower is better if using distance, higher is better if using similarity)\n",
    "            all_results = []\n",
    "            for doc, score in text_results:\n",
    "                doc_id = doc.metadata.get(self.id_key)\n",
    "                all_results.append({\n",
    "                    \"modality\": doc.metadata.get(\"type\"),\n",
    "                    \"summary\": doc.page_content,\n",
    "                    \"original_metadata\": doc.metadata,\n",
    "                    \"score\": score,\n",
    "                    \"doc_id\": doc_id\n",
    "                })\n",
    "            for doc, score in image_results:\n",
    "                doc_id = doc.metadata.get(self.id_key)\n",
    "                all_results.append({\n",
    "                    \"modality\": doc.metadata.get(\"type\"),\n",
    "                    \"summary\": doc.page_content,\n",
    "                    \"original_metadata\": doc.metadata,\n",
    "                    \"score\": score,\n",
    "                    \"doc_id\": doc_id\n",
    "                })\n",
    "            # Sort by score (descending if similarity, ascending if distance)\n",
    "            all_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "            return all_results\n",
    "\n",
    "    persist_directory = '../../chroma/Ears'\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    open_clip_embeddings = OpenCLIPEmbeddings(model_name=\"ViT-g-14\", checkpoint=\"laion2b_s34b_b88k\")\n",
    "\n",
    "    # Vectorstore for summaries (for similarity search)\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"summaries_and_images\",\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=open_clip_embeddings\n",
    "    )\n",
    "    # Persistent docstore for originals (all modalities)\n",
    "    docstore = Chroma(\n",
    "        collection_name=\"originals\",\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=open_clip_embeddings\n",
    "    )\n",
    "\n",
    "    retriever = UnifiedRetriever(vectorstore, docstore, id_key=id_key)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume decomposed_queries is a list of query strings\n",
    "# and retriever is already instantiated\n",
    "\n",
    "seen_doc_ids = set()\n",
    "all_results = []\n",
    "retriever = init_retriever()\n",
    "\n",
    "def contextual_retrieval_flat(state):\n",
    "    seen_doc_ids = set()\n",
    "    unique_docs = []\n",
    "    retriever = init_retriever() \n",
    "\n",
    "    for query in state['sub_queries']:\n",
    "        results = retriever.retrieve_multi_modal(query, k=5, )\n",
    "        for res in results:\n",
    "            doc_id = res.get('doc_id') or res.get('summary_metadata', {}).get('doc_id')\n",
    "            if doc_id and doc_id not in seen_doc_ids:\n",
    "                seen_doc_ids.add(doc_id)\n",
    "                unique_docs.append(res)\n",
    "    print(f\"Total unique documents retrieved: {len(unique_docs)}\")\n",
    "    return {\"retrieved_docs\": unique_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique documents retrieved: 26\n",
      "\n",
      "Sample of unique retrieved docs:\n",
      "Doc 0:\n",
      "  Doc ID: 19b09196-5c83-493e-b0c2-f2a932daec2f\n",
      "  Type: image\n",
      "  Score: 1.1225041151046753\n",
      "  Summary: ./figures/Ears/figure-2-2.jpg...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 1:\n",
      "  Doc ID: 5c1b30a0-1ee7-4cf6-9739-449565ffaebe_context\n",
      "  [IMAGE CONTEXT] Points to image file: None\n",
      "  Type: image_summary\n",
      "  Score: 1.1118836402893066\n",
      "  Summary: The provided local text indicates that this image is part of an educational or informative series fo...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 2:\n",
      "  Doc ID: cc9ca569-c024-4987-8679-3a64d478b74a_context\n",
      "  [IMAGE CONTEXT] Points to image file: None\n",
      "  Type: image_summary\n",
      "  Score: 1.0678699016571045\n",
      "  Summary: The image shows the ear of a cat displaying signs of hematoma, which is characterized by swelling an...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 3:\n",
      "  Doc ID: c97612b4-4a14-4333-b828-59ef2e6d20e8_context\n",
      "  [IMAGE CONTEXT] Points to image file: None\n",
      "  Type: image_summary\n",
      "  Score: 1.0204668045043945\n",
      "  Summary: The image shows a cat with its head wrapped in what appears to be a bandage, likely for protection a...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 4:\n",
      "  Doc ID: de086559-946a-4b51-9c31-d30e27dd035f_context\n",
      "  [IMAGE CONTEXT] Points to image file: None\n",
      "  Type: image_summary\n",
      "  Score: 0.8243616223335266\n",
      "  Summary: This image is not directly related to veterinary content. It appears to be an illustration of a cat ...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 5:\n",
      "  Doc ID: 68ab2b9c-ff2e-43da-92ee-8349dc8dda06\n",
      "  Type: text\n",
      "  Score: 0.6132097840309143\n",
      "  Summary: Benign ceruminous gland cysts occur in cats' ears, appearing as dark lesions or clusters resembling ...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 6:\n",
      "  Doc ID: 20431ed9-e5a4-49dc-a385-c65f857ff476\n",
      "  Type: text\n",
      "  Score: 0.5772411227226257\n",
      "  Summary: Key information:\n",
      "\n",
      "* Otitis interna is an inner ear infection that can start as a middle ear infectio...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 7:\n",
      "  Doc ID: cd66810a-2a7e-4a9d-8ee0-2c88671b37bf\n",
      "  Type: text\n",
      "  Score: 0.5220381021499634\n",
      "  Summary: Cats frequently suffer from painful bites and scratches, especially around the ear (pinna), which ca...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 8:\n",
      "  Doc ID: c155b851-c3d8-45b3-9509-6f29dba41452\n",
      "  Type: text\n",
      "  Score: 0.4894971251487732\n",
      "  Summary: Key signs of an ear problem in cats include:\n",
      "\n",
      "* Ear scratching or shaking\n",
      "* Bad odor from the ear\n",
      "* ...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 9:\n",
      "  Doc ID: 71b93f03-d93f-458d-9f1f-14d7a1d3f8d4\n",
      "  Type: text\n",
      "  Score: 0.4252076745033264\n",
      "  Summary: Otitis Media in cats is rare and usually caused by an external ear infection that ruptures the eardr...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 10:\n",
      "  Doc ID: ba3b992a-476a-445b-8706-608fea8a8ac9_context\n",
      "  [IMAGE CONTEXT] Points to image file: None\n",
      "  Type: image_summary\n",
      "  Score: 0.9984230995178223\n",
      "  Summary: The image depicts an individual applying medication into the ear canal of what appears to be a cat. ...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 11:\n",
      "  Doc ID: 528a70c2-4030-4712-abf1-8dc979f61282\n",
      "  Type: text\n",
      "  Score: 0.6135373115539551\n",
      "  Summary: Here is a concise summary:\n",
      "\n",
      "**Bacterial Otitis Externa in Cats**\n",
      "\n",
      "Causes: scratches, bites, excess w...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 12:\n",
      "  Doc ID: 4f7562ff-0fb2-49b0-be9b-b7f4afe42ba2\n",
      "  Type: text\n",
      "  Score: 0.5868130922317505\n",
      "  Summary: Here's a concise summary:\n",
      "\n",
      "Ear polyps in cats are growths primarily seen in cats between 1-4 years o...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 13:\n",
      "  Doc ID: fcd84514-d8aa-4e17-aa93-d0b171ffba40\n",
      "  Type: text\n",
      "  Score: 0.7432873249053955\n",
      "  Summary: Here's a concise summary:\n",
      "\n",
      "Older cats may experience gradual hearing loss, but can still hear high-p...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 14:\n",
      "  Doc ID: de086559-946a-4b51-9c31-d30e27dd035f\n",
      "  Type: image\n",
      "  Score: 1.5841870307922363\n",
      "  Summary: ./figures/Ears/figure-3-4.jpg...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 15:\n",
      "  Doc ID: 9ac92912-1ced-4d3d-a89b-30cb4aded554\n",
      "  Type: image\n",
      "  Score: 1.5780986547470093\n",
      "  Summary: ./figures/Ears/figure-4-6.jpg...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 16:\n",
      "  Doc ID: 9a488c79-c0a4-449e-8bf9-52db769a8331\n",
      "  Type: image\n",
      "  Score: 1.5718364715576172\n",
      "  Summary: ./figures/Ears/figure-4-7.jpg...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 17:\n",
      "  Doc ID: bc4669a8-0ee8-4549-a0e3-4fb3f79b8403\n",
      "  Type: text\n",
      "  Score: 1.3843967914581299\n",
      "  Summary: Injuries from bites and lacerations can cause infections, so an ear bandage may be necessary to prev...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 18:\n",
      "  Doc ID: e883f5ea-b745-457f-bb33-83120a597e87\n",
      "  Type: text\n",
      "  Score: 1.3654224872589111\n",
      "  Summary: Here's a concise summary:\n",
      "\n",
      "Sudden swelling around the ear can be caused by either an abscess or a he...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 19:\n",
      "  Doc ID: a9f3ee5c-e7e8-495d-a6a9-e77438fe41e7\n",
      "  Type: text\n",
      "  Score: 1.3409104347229004\n",
      "  Summary: EAR ALLERGIES: Itching and skin redness without discharge, often presenting as otitis. May be caused...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 20:\n",
      "  Doc ID: ffe90423-13a4-4f72-9c9c-13014d5e4ef0\n",
      "  Type: text\n",
      "  Score: 1.328994870185852\n",
      "  Summary: Ear canal signs include:\n",
      "\n",
      "* Discharge\n",
      "* Shaking the head\n",
      "* Scratching/pawing at the ear\n",
      "\n",
      "Causes of i...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 21:\n",
      "  Doc ID: d445e457-248b-47fc-91c5-ae91381e5077\n",
      "  Type: text\n",
      "  Score: 1.3037266731262207\n",
      "  Summary: Here's a concise summary:\n",
      "\n",
      "Yeast or fungal otitis externa can develop due to prolonged antibiotic us...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 22:\n",
      "  Doc ID: 9ac92912-1ced-4d3d-a89b-30cb4aded554_context\n",
      "  [IMAGE CONTEXT] Points to image file: None\n",
      "  Type: image_summary\n",
      "  Score: 0.9138299226760864\n",
      "  Summary: The image shows a veterinary assistant administering Epi-Otic to a cat's ear. The context suggests t...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 23:\n",
      "  Doc ID: 9a488c79-c0a4-449e-8bf9-52db769a8331_context\n",
      "  [IMAGE CONTEXT] Points to image file: None\n",
      "  Type: image_summary\n",
      "  Score: 0.9063699245452881\n",
      "  Summary: The image shows a veterinary assistant performing an ear cleaning procedure on a cat. The veterinari...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 24:\n",
      "  Doc ID: 1b1f1d68-7454-4710-a03f-b5ba50f6e8c0\n",
      "  Type: text\n",
      "  Score: 0.698249876499176\n",
      "  Summary: Apply ear medication to clean ear canals only. Discuss with vet on suitable cleaning solution and us...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Doc 25:\n",
      "  Doc ID: 61dc1297-3bd3-43c9-adcc-63782f77f690\n",
      "  Type: text\n",
      "  Score: 0.6866337060928345\n",
      "  Summary: To prevent ear infections and maintain ear health:\n",
      "\n",
      "* Bathe cat avoiding water in ears with cotton b...\n",
      "  Original: None...\n",
      "----------------------------------------\n",
      "Total unique docs retrieved: 26\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def test_contextual_retrieval(sub_queries):\n",
    "    global test_retrived_doc\n",
    "\n",
    "    test_state = {\n",
    "        \"sub_queries\": sub_queries\n",
    "    }\n",
    "\n",
    "    # Use the flat contextual retrieval function\n",
    "    new_state = contextual_retrieval_flat(test_state)\n",
    "    unique_docs = new_state[\"retrieved_docs\"]\n",
    "    print(\"\\nSample of unique retrieved docs:\")\n",
    "    for i, doc in enumerate(unique_docs):\n",
    "        doc_id = doc.get('doc_id') or doc.get('summary_metadata', {}).get('doc_id')\n",
    "        print(f\"Doc {i}:\")\n",
    "        print(f\"  Doc ID: {doc_id}\")\n",
    "        # Check if this is an image context doc\n",
    "        if doc_id and doc_id.endswith('_context'):\n",
    "            image_path = doc.get('summary_metadata', {}).get('image_path')\n",
    "            print(f\"  [IMAGE CONTEXT] Points to image file: {image_path}\")\n",
    "        print(f\"  Type: {(doc.get('original_metadata') or {}).get('type')}\")\n",
    "        print(f\"  Score: {doc.get('score')}\")\n",
    "        print(f\"  Summary: {doc.get('summary')[:100]}...\")\n",
    "        print(f\"  Original: {str(doc.get('original'))[:100]}...\")\n",
    "        print(\"-\" * 40)\n",
    "    print(f\"Total unique docs retrieved: {len(unique_docs)}\")\n",
    "    \n",
    "    test_retrived_doc = copy.deepcopy(unique_docs)\n",
    "\n",
    "# Example usage:\n",
    "test_contextual_retrieval(\n",
    "   ['What are common causes of chronic ear infections or otitis in cats?', 'Describe diagnostic steps for detecting chronic ear infections or otitis in cats', 'What is the significance of visible brownish-orange debris, discharge, and wounds in the ear canal in cats?', 'How can redness, swelling, and bleeding be distinguished from other feline health issues affecting the auditory system?', 'What are potential complications if left untreated?', 'Show me an image of a cat with signs of otitis externa', \"Show me an image of how to properly clean a cat's ear canal\", 'Describe the anatomy of the feline ear canal']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReRank\n",
    "\n",
    "Retrievals returns docs with high similarities based on cosine-similarity. However, we do need to re-rank their improtance on contexual level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting image, image_summary pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truly multimodel [monoqwen], use here If running on Nvidia GPU Machine\n",
    "# pip install \"rerankers[monovlm]\" qwen-vl-utils transformers\n",
    "from rerankers import MonoQwen2VLReranker\n",
    "\n",
    "def rerank_node_monoqwen(state):\n",
    "    query = state['refined_query']\n",
    "    candidates = state['retrieved_docs']\n",
    "\n",
    "    # Prepare candidates for reranker\n",
    "    rerank_inputs = []\n",
    "    for doc in candidates:\n",
    "        if doc.get(\"modality\") == \"text\":\n",
    "            rerank_inputs.append(doc[\"summary\"])\n",
    "        elif doc.get(\"modality\") in (\"image\", \"image_summary\"):\n",
    "            # Use image path if available, else fallback to summary\n",
    "            image_path = doc.get(\"original_metadata\", {}).get(\"image_path\")\n",
    "            if image_path:\n",
    "                rerank_inputs.append(image_path)\n",
    "            else:\n",
    "                rerank_inputs.append(doc[\"summary\"])\n",
    "        else:\n",
    "            rerank_inputs.append(doc[\"summary\"])\n",
    "\n",
    "    # Rerank\n",
    "    from rerankers import MonoQwen2VLReranker\n",
    "    reranker = MonoQwen2VLReranker.from_pretrained(\"Qwen/MonoQwen2-VL-v0.1\")\n",
    "    results = reranker.rerank(query, rerank_inputs, top_k=len(rerank_inputs))\n",
    "\n",
    "    # Attach scores and sort\n",
    "    for (idx, score) in results:\n",
    "        candidates[idx]['rerank_score'] = float(score)\n",
    "    reranked = sorted(candidates, key=lambda x: x.get('rerank_score', 0), reverse=True)\n",
    "    return {\"reranked_docs\": reranked}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jina Reranker m0. GPU/CPU, but extremly slow in CPU\n",
    "import base64\n",
    "import os\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "def image_to_base64(image_path):\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def rerank_node_jina_vlm(state):\n",
    "    query = state['refined_query']\n",
    "    candidates = state['retrieved_docs']\n",
    "\n",
    "    documents = []\n",
    "    doc_types = []\n",
    "    for doc in candidates:\n",
    "        if doc.get(\"modality\") in (\"image\", \"image_summary\"):\n",
    "            image_path = doc.get(\"original_metadata\", {}).get(\"image_path\")\n",
    "            if image_path and os.path.exists(image_path):\n",
    "                documents.append(image_to_base64(image_path))\n",
    "                doc_types.append(\"image\")\n",
    "            else:\n",
    "                documents.append(doc[\"summary\"])\n",
    "                doc_types.append(\"text\")\n",
    "        else:\n",
    "            documents.append(doc[\"summary\"])\n",
    "            doc_types.append(\"text\")\n",
    "\n",
    "    pairs = [[query, doc] for doc in documents]\n",
    "\n",
    "    model = AutoModel.from_pretrained(\n",
    "        'jinaai/jina-reranker-m0',\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "\n",
    "    # If most docs are images, use doc_type=\"image\", else \"text\"\n",
    "    n_images = doc_types.count(\"image\")\n",
    "    n_texts = doc_types.count(\"text\")\n",
    "    doc_type = \"image\" if n_images > n_texts else \"text\"\n",
    "\n",
    "    # If mixed, filter and rerank separately, then merge (advanced)\n",
    "    # For now, just use the dominant type\n",
    "    scores = model.compute_score(pairs, max_length=2048, doc_type=doc_type)\n",
    "\n",
    "    for doc, score in zip(candidates, scores):\n",
    "        doc['rerank_score'] = float(score)\n",
    "    reranked = sorted(candidates, key=lambda x: x.get('rerank_score', 0), reverse=True)\n",
    "    return {\"reranked_docs\": reranked}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Method: CrossEncoder for text, VLM for image.\n",
    "from sentence_transformers import CrossEncoder\n",
    "import os\n",
    "import ollama\n",
    "def llm_image_relevance_score(query, image_path, image_summary=None):\n",
    "    \"\"\"\n",
    "    Use Ollama (minicpm-v:8b) to rate the relevance of an image to the query.\n",
    "    Passes the image file and, if available, the image summary.\n",
    "    Returns a float score between 0 and 1.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a veterinary assistant AI. Given the following user query and an image, rate how relevant the image is to answering the query.\n",
    "    - User Query: \"{query}\"\n",
    "    \"\"\"\n",
    "    if image_summary:\n",
    "        prompt += f'- Image Summary: \"{image_summary}\"\\n'\n",
    "    prompt += \"Respond with a single float between 0 (not relevant at all) and 1 (highly relevant). Only output the number, nothing else.\"\n",
    "\n",
    "    try:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if image_path and os.path.exists(image_path):\n",
    "            messages[0][\"images\"] = [image_path]\n",
    "        response = ollama.chat(\n",
    "            model=\"minicpm-v:8b\",\n",
    "            messages=messages,\n",
    "            options={\"temperature\": 0.0}\n",
    "        )\n",
    "        content = response['message']['content'].strip()\n",
    "        score = float(content.split()[0])\n",
    "        score = max(0.0, min(1.0, score))\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"[llm_image_relevance_score] Error: {e}. Query: {query[:50]}... Image: {image_path}... Summary: {str(image_summary)[:50]}...\")\n",
    "        return 0.0\n",
    "\n",
    "def rerank_node_hybrid_v2(state):\n",
    "    query = state['refined_query']\n",
    "    candidates = state['retrieved_docs']\n",
    "\n",
    "    text_indices = []\n",
    "    text_contents = []\n",
    "    image_indices = []\n",
    "    image_info = []\n",
    "\n",
    "    for idx, doc in enumerate(candidates):\n",
    "        modality = doc.get(\"modality\") or (doc.get(\"original_metadata\") or {}).get(\"type\")\n",
    "        if modality == \"text\":\n",
    "            text_indices.append(idx)\n",
    "            doc_id = (doc.get(\"original_metadata\") or {}).get(\"doc_id\") or doc.get(\"doc_id\")\n",
    "            # Fetch the original document from the docstore\n",
    "            original_text = None\n",
    "            if doc_id and docstore:\n",
    "                try:\n",
    "                    original = docstore._collection.get(ids=[doc_id], include=[\"documents\"])\n",
    "                    original_text = original[\"documents\"][0] if original[\"documents\"] else None\n",
    "                except Exception as e:\n",
    "                    print(f\"[rerank_node_hybrid_v2] Error fetching original text for doc_id {doc_id}: {e}\")\n",
    "            if not original_text:\n",
    "                original_text = doc.get(\"summary\", \"\")\n",
    "            text_contents.append(original_text)\n",
    "        elif modality == \"image\":\n",
    "            # Use the image file for VLM\n",
    "            image_path = (doc.get(\"original_metadata\") or {}).get(\"image_path\")\n",
    "            image_summary = (doc.get(\"original_metadata\") or {}).get(\"summary\", \"\")\n",
    "            image_indices.append(idx)\n",
    "            image_info.append((image_path, image_summary if image_summary else None))\n",
    "        elif modality == \"image_summary\":\n",
    "            # Trace to the image file if possible\n",
    "            image_path = (doc.get(\"original_metadata\") or {}).get(\"image_path\")\n",
    "            image_summary = doc.get(\"summary\", \"\")\n",
    "            image_indices.append(idx)\n",
    "            image_info.append((image_path, image_summary))\n",
    "        else:\n",
    "            # Fallback: treat as text\n",
    "            text_indices.append(idx)\n",
    "            text_contents.append(doc.get(\"summary\", \"\"))\n",
    "\n",
    "    # 1. Rerank text docs\n",
    "    if text_contents:\n",
    "        model = CrossEncoder(\"BAAI/bge-reranker-base\")\n",
    "        pairs = [(query, text) for text in text_contents]\n",
    "        scores = model.predict(pairs)\n",
    "        for idx, score in zip(text_indices, scores):\n",
    "            candidates[idx]['rerank_score'] = float(score)\n",
    "\n",
    "    # 2. Rerank images (and image summaries) with VLM\n",
    "    for idx, (image_path, image_summary) in zip(image_indices, image_info):\n",
    "        score = llm_image_relevance_score(query, image_path, image_summary)\n",
    "        candidates[idx]['rerank_score'] = float(score)\n",
    "\n",
    "    # 3. Sort all by rerank_score\n",
    "    reranked = sorted(candidates, key=lambda x: x.get('rerank_score', 0), reverse=True)\n",
    "    return {\"reranked_docs\": reranked}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rerank_node(rerank_node, refined_query, retrieved_docs, top_n=5):\n",
    "    # Build the state as expected by rerank_node\n",
    "    state = {\n",
    "      \"refined_query\": refined_query,\n",
    "      \"retrieved_docs\": retrieved_docs,\n",
    "      \"docstore\": retriever.docstore  # or whatever your docstore object is\n",
    "     }\n",
    "    # Call the rerank node\n",
    "    new_state = rerank_node(state)\n",
    "    global reranked_docs\n",
    "    reranked_docs = new_state.get(\"reranked_docs\", [])\n",
    "    print(f\"Total docs after reranking: {len(reranked_docs)}\")\n",
    "    print(f\"Top {top_n} reranked docs (by rerank_score):\")\n",
    "    for i, doc in enumerate(reranked_docs[:top_n]):\n",
    "        doc_id = doc.get('doc_id') or (doc.get('original_metadata') or {}).get('doc_id')\n",
    "        modality = (doc.get('original_metadata') or {}).get('type') or doc.get('modality')\n",
    "        print(f\"Doc {i}:\")\n",
    "        print(f\"  Doc ID: {doc_id}\")\n",
    "        print(f\"  Type: {modality}\")\n",
    "        print(f\"  Rerank Score: {doc.get('rerank_score')}\")\n",
    "        print(f\"  Summary: {doc.get('summary')[:100]}...\")\n",
    "        print(\"-\" * 40)\n",
    "    # Optionally, check that scores are sorted descending\n",
    "    scores = [doc.get('rerank_score') for doc in reranked_docs if doc.get('rerank_score') is not None]\n",
    "    if scores and scores == sorted(scores, reverse=True):\n",
    "        print(\"PASS: Docs are sorted by rerank_score descending.\")\n",
    "    else:\n",
    "        print(\"FAIL: Docs are not sorted correctly or scores are missing.\")\n",
    "\n",
    "# Example usage:\n",
    "refined_query = \"What are possible causes and diagnostic steps for chronic ear infections or otitis in cats, characterized by visible brownish-orange debris, discharge, and wounds in the ear canal, potentially accompanied by signs of infection such as redness, swelling, and bleeding, and how can these conditions be distinguished from other potential health issues that may affect a cat's auditory system\"\n",
    "test_rerank_node(rerank_node_hybrid_v2, refined_query, test_retrived_doc)\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Images and Original Texts:\n",
      "\n",
      "Doc 0:\n",
      "  Doc ID: c155b851-c3d8-45b3-9509-6f29dba41452\n",
      "  Type: text\n",
      "  Rerank Score: 0.9639337062835693\n",
      "  Original Text:\n",
      "    Structure of the Ears 206 ‚Ä¢ CAT OWNER‚ÄôS HOME VETERINARY HANDBOOK Your cat has an ear problem if you notice ear scratching, repeated head shaking, a bad odor emanating from the ear, or large amounts of waxy dis- charge or pus draining. In a younger cat, the most likely cause is ear mites, but other diseases of the ears (such as allergies) do occur. Diseases of the middle ear cause head tilt and the loss of hearing. Diseases of the inner ear affect the balance center. The cat wobbles, circles, fal...\n",
      "------------------------------------------------------------\n",
      "Doc 1:\n",
      "  Doc ID: cd66810a-2a7e-4a9d-8ee0-2c88671b37bf\n",
      "  Type: text\n",
      "  Rerank Score: 0.9461669325828552\n",
      "  Original Text:\n",
      "    BITES AND LACERATIONS Cats give and receive painful bites and scratches that are prone to severe infec- tion. The pinna is a frequent site for such injuries. Some occur during mating.\n",
      "------------------------------------------------------------\n",
      "Doc 2:\n",
      "  Doc ID: 71b93f03-d93f-458d-9f1f-14d7a1d3f8d4\n",
      "  Type: text\n",
      "  Rerank Score: 0.8675187826156616\n",
      "  Original Text:\n",
      "    Otitis Media This condition, a middle ear infection, is not common in cats. Most cases result from an external ear infection that ruptures the eardrum. Tonsillitis and mouth and sinus infections can travel to the middle ear through the Eustachian tube, a passage that connects the middle ear to the back of the throat. Rarely, bacteria gain entrance through the bloodstream. ‚Äîo‚Äî\n",
      "------------------------------------------------------------\n",
      "Doc 3:\n",
      "  Doc ID: ba3b992a-476a-445b-8706-608fea8a8ac9_context\n",
      "  Type: image_summary\n",
      "  Rerank Score: 0.85\n",
      "  Image Path: ./figures/Ears/figure-6-9.jpg\n",
      "  Image Summary:\n",
      "    The image depicts an individual applying medication into the ear canal of what appears to be a cat. The context provided suggests this is part of a veterinary handbook, specifically instructing how to apply ointment or instill drops in a pet's ear for treatment purposes. This demonstrates proper technique and care when administering medications directly into a pet‚Äôs ear canal.\n",
      "------------------------------------------------------------\n",
      "Doc 4:\n",
      "  Doc ID: 9a488c79-c0a4-449e-8bf9-52db769a8331\n",
      "  Type: image\n",
      "  Rerank Score: 0.85\n",
      "  Image Path: ./figures/Ears/figure-4-7.jpg\n",
      "  Image Summary:\n",
      "    ./figures/Ears/figure-4-7.jpg\n",
      "------------------------------------------------------------\n",
      "Doc 5:\n",
      "  Doc ID: 9a488c79-c0a4-449e-8bf9-52db769a8331_context\n",
      "  Type: image_summary\n",
      "  Rerank Score: 0.85\n",
      "  Image Path: ./figures/Ears/figure-4-7.jpg\n",
      "  Image Summary:\n",
      "    The image shows a veterinary assistant performing an ear cleaning procedure on a cat. The veterinarian is using their fingers to gently hold down the cat's ear while another hand appears to be applying or massaging an ear-cleaning solution into the base of the cat's ear canal, as described in the local text context provided. This demonstrates proper technique for maintaining cleanliness and health within a cat's ears.\n",
      "------------------------------------------------------------\n",
      "Doc 6:\n",
      "  Doc ID: 528a70c2-4030-4712-abf1-8dc979f61282\n",
      "  Type: text\n",
      "  Rerank Score: 0.7750948071479797\n",
      "  Original Text:\n",
      "    BACTERIAL OTITIS EXTERNA Bacterial infections in the ear canal are frequently caused by scratches to the skin or cat bites. Some begin in an ear canal that contains excessive amounts of wax, cellular debris, or foreign material. Ear mite infections are often the cause of bacterial otitis. Signs of an infected ear canal are shaking the head, scratching at the affected ear, and an unpleasant odor. The cat may tilt or carry her head down on the painful side and exhibit tenderness when the ear is to...\n",
      "------------------------------------------------------------\n",
      "Doc 7:\n",
      "  Doc ID: ffe90423-13a4-4f72-9c9c-13014d5e4ef0\n",
      "  Type: text\n",
      "  Rerank Score: 0.7660390734672546\n",
      "  Original Text:\n",
      "    The Ear Canal Signs of irritation or infection in the ear canals are discharge, shaking the head, and scratching and pawing at the ear. Common causes are listed here.\n",
      "------------------------------------------------------------\n",
      "Doc 8:\n",
      "  Doc ID: 68ab2b9c-ff2e-43da-92ee-8349dc8dda06\n",
      "  Type: text\n",
      "  Rerank Score: 0.7191415429115295\n",
      "  Original Text:\n",
      "    CERUMINOUS GLAND PROBLEMS Benign ceruminous (wax) gland cysts are not uncommon in cats‚Äô ears. These cysts appear as dark lesions throughout the ear. They may cluster and look like a bunch of grapes. If these start to block the ear canal, they should be removed; otherwise they do not generally cause a problem. Cats are also susceptible to ceruminous gland tumors in their ears, which are often malignant adenocarcinomas. These need to be distinguished from ear polyps.\n",
      "------------------------------------------------------------\n",
      "Doc 9:\n",
      "  Doc ID: 61dc1297-3bd3-43c9-adcc-63782f77f690\n",
      "  Type: text\n",
      "  Rerank Score: 0.6807350516319275\n",
      "  Original Text:\n",
      "    Basic Ear Care If you bathe your cat, prevent water from getting into her ears by inserting cotton balls at the opening of the ear canals. Wet ear canals can predispose a cat to ear infections. If your cat has been in a fight, check the ears for any cuts or bites that may need to be treated (see The Pinna, page 209). Routine ear cleaning is not required. Some wax is necessary to maintain the health of the tissues. However, ears should be cleaned when there is an\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def display_top_10_imgs_and_texts(reranked_docs, docstore):\n",
    "    print(\"Top 10 Images and Original Texts:\\n\")\n",
    "    count = 0\n",
    "    for doc in reranked_docs:\n",
    "        if count >= 10:\n",
    "            break\n",
    "        modality = (doc.get('original_metadata') or {}).get('type') or doc.get('modality')\n",
    "        doc_id = doc.get('doc_id') or (doc.get('original_metadata') or {}).get('doc_id')\n",
    "        score = doc.get('rerank_score')\n",
    "        print(f\"Doc {count}:\")\n",
    "        print(f\"  Doc ID: {doc_id}\")\n",
    "        print(f\"  Type: {modality}\")\n",
    "        print(f\"  Rerank Score: {score}\")\n",
    "        if modality == \"text\":\n",
    "            # Fetch original text from docstore\n",
    "            original_text = None\n",
    "            if doc_id and docstore:\n",
    "                try:\n",
    "                    original = docstore._collection.get(ids=[doc_id], include=[\"documents\"])\n",
    "                    original_text = original[\"documents\"][0] if original[\"documents\"] else None\n",
    "                except Exception as e:\n",
    "                    print(f\"    [Error fetching original text for doc_id {doc_id}: {e}]\")\n",
    "            if not original_text:\n",
    "                original_text = doc.get(\"summary\", \"\")\n",
    "            print(\"  Original Text:\")\n",
    "            print(f\"    {original_text[:500]}{'...' if len(original_text) > 500 else ''}\")\n",
    "        elif modality in (\"image\", \"image_summary\"):\n",
    "            image_path = (doc.get(\"original_metadata\") or {}).get(\"image_path\")\n",
    "            print(f\"  Image Path: {image_path}\")\n",
    "            print(\"  Image Summary:\")\n",
    "            print(f\"    {doc.get('summary', '')[:500]}{'...' if len(doc.get('summary', '')) > 500 else ''}\")\n",
    "        else:\n",
    "            print(\"  [Unknown modality]\")\n",
    "        print(\"-\" * 60)\n",
    "        count += 1\n",
    "\n",
    "# Example usage:\n",
    "display_top_10_imgs_and_texts(reranked_docs, retriever.docstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking Node\n",
    "\n",
    "This step is to take all on-hand info and reranked doc to make analysis. Think about user's intent, what they want to know, what they need to know, also what AI need to know."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}