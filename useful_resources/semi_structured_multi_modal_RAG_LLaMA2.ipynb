{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain langchain-chroma \"unstructured[all-docs]\" pydantic lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Source Files using Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Path to save images\n",
    "# path = \"./figures/shortExample1/\"\n",
    "\n",
    "# Get elements\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename='./data/shortExample1P_PickupCat.pdf',\n",
    "    languages=['eng'],\n",
    "    strategy='hi_res',\n",
    "    # Using pdf format to find embedded image blocks\n",
    "    extract_images_in_pdf=True,\n",
    "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
    "    # Titles are any sub-section of the document\n",
    "    infer_table_structure=True,\n",
    "    # Post processing to aggregate text once we have the title\n",
    " \n",
    "    # Chunking params to aggregate text blocks\n",
    "    # Attempt to create a new chunk 3800 chars\n",
    "    # Attempt to keep chunks > 2000 chars\n",
    "    # Hard max on chunks   \n",
    "\n",
    "\n",
    "    # --- Unstructure can't do semantic chunking, it only does fixed sized and by_title. \n",
    "    # --- I will use langChain semantic chunker for that.\n",
    "    \n",
    "    # ----If letting it to do chunking---\n",
    "    # chunking_strategy=\"by_title\",\n",
    "    # max_characters=4000,\n",
    "    # new_after_n_chars=3800,\n",
    "    # combine_text_under_n_chars=2000,\n",
    "    # ------------------------------\n",
    "\n",
    "    image_output_dir_path=\"./figures/shortExample1/\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"<class 'unstructured.documents.elements.Text'>\": 8,\n",
       " \"<class 'unstructured.documents.elements.Header'>\": 3,\n",
       " \"<class 'unstructured.documents.elements.ListItem'>\": 3,\n",
       " \"<class 'unstructured.documents.elements.Title'>\": 8,\n",
       " \"<class 'unstructured.documents.elements.NarrativeText'>\": 16,\n",
       " \"<class 'unstructured.documents.elements.Image'>\": 10,\n",
       " \"<class 'unstructured.documents.elements.FigureCaption'>\": 7}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "# TableChunk if Table > max chars set above\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Partitioned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables extracted: 0\n",
      "Number of text elements for semantic chunking: 9\n",
      "Number of images detected: 10\n",
      "Number of headers: 3\n",
      "Number of titles: 8\n",
      "Number of footers (ignored for RAG): 0\n",
      "Number of figure captions: 7\n",
      "Number of list items: 3\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "\n",
    "# Re-defining Element class with optional context field and original_index\n",
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "    context: Optional[str] = None\n",
    "    original_index: Optional[int] = None # Added to store original position\n",
    "\n",
    "# Initialize lists for categorized elements\n",
    "text_for_semantic_chunking = []\n",
    "tables_raw = []\n",
    "images_raw = []\n",
    "headers_raw = []\n",
    "titles_raw = []\n",
    "footers_raw = []\n",
    "figure_captions_raw = []\n",
    "list_items_raw = []\n",
    "\n",
    "# Variables to build coherent text blocks and manage local context\n",
    "current_text_block = \"\"\n",
    "current_context_prefix = \"\" # This will capture the most recent Header/Title\n",
    "min_meaningful_text_length = 20 # Minimum length for a text block to be considered meaningful\n",
    "\n",
    "# Helper to finalize and append current text block - defined globally\n",
    "def finalize_text_block():\n",
    "    global current_text_block\n",
    "    global current_context_prefix\n",
    "    if current_text_block.strip() and len(current_text_block.strip()) >= min_meaningful_text_length:\n",
    "        # No debug prints in final version of this helper\n",
    "        text_for_semantic_chunking.append(Element(type=\"text\", text=current_text_block.strip()))\n",
    "    current_text_block = \"\" # Reset for next block\n",
    "\n",
    "for i, element in enumerate(raw_pdf_elements):\n",
    "    element_type_str = str(type(element))\n",
    "    element_text = str(element).strip()\n",
    "\n",
    "    if \"unstructured.documents.elements.Header\" in element_type_str:\n",
    "        finalize_text_block()\n",
    "        \n",
    "        is_running_header = False\n",
    "        lower_element_text = element_text.lower()\n",
    "        if (\n",
    "            \"qxp\" in lower_element_text or\n",
    "            \"pm\" in lower_element_text or\n",
    "            \"am\" in lower_element_text or\n",
    "            \"page\" in lower_element_text or\n",
    "            re.search(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', lower_element_text)\n",
    "        ):\n",
    "            is_running_header = True\n",
    "\n",
    "        if not is_running_header:\n",
    "            current_context_prefix = element_text + \" \"\n",
    "        headers_raw.append(Element(type=\"header\", text=element_text, original_index=i))\n",
    "    elif \"unstructured.documents.elements.Title\" in element_type_str:\n",
    "        finalize_text_block()\n",
    "        current_context_prefix = element_text + \" \"\n",
    "        titles_raw.append(Element(type=\"title\", text=element_text, original_index=i))\n",
    "    elif \"unstructured.documents.elements.NarrativeText\" in element_type_str or \\\n",
    "         \"unstructured.documents.elements.ListItem\" in element_type_str or \\\n",
    "         \"unstructured.documents.elements.Text\" in element_type_str:\n",
    "        \n",
    "        if len(element_text) < 5 and not any(char.isalpha() for char in element_text):\n",
    "            continue\n",
    "\n",
    "        if not current_text_block and current_context_prefix:\n",
    "            current_text_block += current_context_prefix\n",
    "            \n",
    "        current_text_block += element_text + \" \"\n",
    "        # We don't add these directly to text_for_semantic_chunking here,\n",
    "        # they are accumulated in current_text_block and added via finalize_text_block()\n",
    "        if \"unstructured.documents.elements.ListItem\" in element_type_str:\n",
    "            list_items_raw.append(Element(type=\"list_item\", text=element_text, original_index=i))\n",
    "\n",
    "    elif \"unstructured.documents.elements.Table\" in element_type_str:\n",
    "        finalize_text_block()\n",
    "        tables_raw.append(Element(type=\"table\", text=element_text, original_index=i))\n",
    "        # current_context_prefix is NOT reset here\n",
    "    elif \"unstructured.documents.elements.Image\" in element_type_str:\n",
    "        finalize_text_block() # Finalize any text block *before* the image\n",
    "\n",
    "        image_path = getattr(element.metadata, \"image_path\", \"N/A\")\n",
    "        # For initial context, we'll assign it a placeholder or previous context.\n",
    "        # The full context will be enriched in a post-processing step.\n",
    "        images_raw.append(Element(type=\"image\", text=image_path, context=\"\", original_index=i))\n",
    "        current_text_block = \"\" # Reset text block after an image\n",
    "\n",
    "    elif \"unstructured.documents.elements.FigureCaption\" in element_type_str:\n",
    "        finalize_text_block() # Finalize current text block before a caption\n",
    "        figure_captions_raw.append(Element(type=\"figure_caption\", text=element_text, original_index=i))\n",
    "        # current_context_prefix is NOT reset here\n",
    "    elif \"unstructured.documents.elements.Footer\" in element_type_str:\n",
    "        footers_raw.append(Element(type=\"footer\", text=element_text, original_index=i))\n",
    "\n",
    "# Finalize any remaining text block after the loop\n",
    "finalize_text_block()\n",
    "\n",
    "\n",
    "# --- New Function for Image Context Enrichment ---\n",
    "def enrich_image_context(images, all_raw_elements, window_size=3):\n",
    "    \"\"\"\n",
    "    Enriches the context for each image by looking at a window of surrounding text and captions.\n",
    "    window_size: Number of elements to look before and after the image.\n",
    "    \"\"\"\n",
    "    for img_element in images:\n",
    "        img_index = img_element.original_index\n",
    "        if img_index is None:\n",
    "            continue\n",
    "\n",
    "        start_index = max(0, img_index - window_size)\n",
    "        end_index = min(len(all_raw_elements), img_index + window_size + 1)\n",
    "        \n",
    "        surrounding_text_elements = []\n",
    "        for j in range(start_index, end_index):\n",
    "            surrounding_element = all_raw_elements[j]\n",
    "            element_type_str = str(type(surrounding_element))\n",
    "            element_text = str(surrounding_element).strip()\n",
    "\n",
    "            # Include NarrativeText, ListItem, Text, and FigureCaption\n",
    "            if \"unstructured.documents.elements.NarrativeText\" in element_type_str or \\\n",
    "               \"unstructured.documents.elements.ListItem\" in element_type_str or \\\n",
    "               \"unstructured.documents.elements.Text\" in element_type_str or \\\n",
    "               \"unstructured.documents.elements.FigureCaption\" in element_type_str:\n",
    "                \n",
    "                # Also, try to get the most recent non-running header/title before this window\n",
    "                # This is implicitly handled by `current_context_prefix` during the initial pass\n",
    "                # but for post-processing, we might need a more direct way if we want to include\n",
    "                # it for elements outside `current_text_block` capture.\n",
    "\n",
    "                # For simplicity here, we'll just concatenate nearby text.\n",
    "                # The assumption is that `current_context_prefix` will have set the general section context.\n",
    "                if len(element_text) >= 5 or any(char.isalpha() for char in element_text): # Avoid very short, non-alphabetic\n",
    "                     surrounding_text_elements.append(element_text)\n",
    "            \n",
    "            # Special handling for Headers/Titles that are not running headers\n",
    "            if \"unstructured.documents.elements.Header\" in element_type_str or \\\n",
    "               \"unstructured.documents.elements.Title\" in element_type_str:\n",
    "                lower_element_text = element_text.lower()\n",
    "                is_running_header = (\n",
    "                    \"qxp\" in lower_element_text or \"pm\" in lower_element_text or\n",
    "                    \"am\" in lower_element_text or \"page\" in lower_element_text or\n",
    "                    re.search(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', lower_element_text)\n",
    "                )\n",
    "                if not is_running_header:\n",
    "                    surrounding_text_elements.append(element_text)\n",
    "\n",
    "\n",
    "        # Combine the relevant surrounding text to form the image context\n",
    "        enriched_context = \" \".join(surrounding_text_elements).strip()\n",
    "        if not enriched_context:\n",
    "            enriched_context = \"No specific text context available around this image.\"\n",
    "        \n",
    "        img_element.context = enriched_context\n",
    "\n",
    "# Call the enrichment function after initial parsing\n",
    "enrich_image_context(images_raw, raw_pdf_elements, window_size=5) # Increased window_size for broader context\n",
    "\n",
    "# Now, 'texts' should contain the cleaned and combined narrative chunks\n",
    "texts = [e.text for e in text_for_semantic_chunking]\n",
    "tables = [e.text for e in tables_raw]\n",
    "\n",
    "print(f\"Number of tables extracted: {len(tables)}\")\n",
    "print(f\"Number of text elements for semantic chunking: {len(texts)}\")\n",
    "print(f\"Number of images detected: {len(images_raw)}\")\n",
    "print(f\"Number of headers: {len(headers_raw)}\")\n",
    "print(f\"Number of titles: {len(titles_raw)}\")\n",
    "print(f\"Number of footers (ignored for RAG): {len(footers_raw)}\")\n",
    "print(f\"Number of figure captions: {len(figure_captions_raw)}\")\n",
    "print(f\"Number of list items: {len(list_items_raw)}\")\n",
    "\n",
    "# You can inspect an image element to see its context:\n",
    "# if images_raw:\n",
    "#     print(\"\\nExample Image Element with Context:\")\n",
    "#     print(images_raw[0].dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text & Table Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Prompt\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "# New: Text Summarization Prompt\n",
    "prompt_text_summary = \"\"\"You are an assistant tasked with concisely summarizing text sections related to veterinary advice and pet care. Focus on key information, main ideas, and any actionable advice. Just give me the summary, be concise and do not be verbose. Text chunk: {element} \"\"\"\n",
    "prompt_text = ChatPromptTemplate.from_template(prompt_text_summary)\n",
    "text_summarize_chain = {\"element\": lambda x: x} | prompt_text | model | StrOutputParser()\n",
    "\n",
    "# New: Table Summarization Prompt\n",
    "prompt_table_summary = \"\"\"You are an assistant tasked with extracting key data, trends, and important numerical information from the provided table, especially when related to pet nutrition, health, or statistics. Just give me the summary, be concise and do not be verbose. Table chunk: {element} \"\"\"\n",
    "prompt_table = ChatPromptTemplate.from_template(prompt_table_summary)\n",
    "table_summarize_chain = {\"element\": lambda x: x} | prompt_table | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply to text\n",
    "# texts = [i.text for i in text_elements if i.text != \"\"]\n",
    "text_summaries = text_summarize_chain.batch(texts, {\"max_concurrency\": 8})\n",
    "\n",
    "# Apply to tables\n",
    "# tables = [i.text for i in table_elements]\n",
    "table_summaries = table_summarize_chain.batch(tables, {\"max_concurrency\": 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Summary Generatetion\n",
    "From the LangChain Cookbook, they used a LLaVA 7B model to generate image summaries in .txt. Those files will be in the same dir as those images. However, I have llama3.2-vision already installed and setup on my local machine. \n",
    "\n",
    "Llama3.2-vision model is a 11B model which may require a strong computing power and large memory. Switch model if necessary. i.e. Llava, Qwen, Gemma, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️Patch:\n",
    "Generic query often won't trigger the LLM to include image in the response. AI analysis shows it could be a problem of how images summaries were created. Retriever performs sematic matching to retrieve relevant chunks, but the current summaries was written in a outsider perspective. It'd be better to try following prompt in making summaries.\n",
    "\n",
    "'content': 'Describe the image in detail, focusing on any actions, techniques, or procedures depicted related to pet handling or care. Explain the purpose or context of the actions shown, if clear. Be concise and relevant to veterinary advice.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Side Note:\n",
    "There are many irrelevant images exist in the textbook, like paragraph divider, section dividers, etc. In future development, consider using a **Node/Agent** to decide if a image should be filtered out for summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Redefine the prompt for image relevance to include image_context\n",
    "image_relevance_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an intelligent assistant. Your task is to determine if an image could be relevant to the provided local textual context within a veterinary handbook. Consider the filename and the immediate context.\"),\n",
    "        (\"human\", \"Local Textual Context: {image_context}\\n\\nImage Filename: {image_filename}\\n\\nIs this image relevant to the content? Respond with 'yes' if relevant, 'no' if not. Only respond with 'yes' or 'no'.\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. The image relevance chain setup remains the same\n",
    "image_relevance_chain = image_relevance_prompt | ChatOllama(model=\"llama3.2-vision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking image relevance with local textual context (now using Ollama's direct image passing)...\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-1-1.jpg\n",
      "DEBUG: Associated text context for model: 'Any cat, no matter how docile he may be, has the potential to bite when he is severely injured, frig...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-1-1.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-1-1.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-1-1.jpg' is RELEVANT.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-1-2.jpg\n",
      "DEBUG: Associated text context for model: 'There are several effective ways to handle and restrain a cat. Your choice will depend on whether th...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-1-2.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-1-2.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-1-2.jpg' is RELEVANT.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-3.jpg\n",
      "DEBUG: Associated text context for model: 'Secure the back feet with your other hand. —o— 04_095300 ch01.qxp 10/29/07 3:41 PM Page 3 2h EMERGEN...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-3.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-2-3.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-3.jpg' is RELEVANT.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-4.jpg\n",
      "DEBUG: Associated text context for model: '04_095300 ch01.qxp 10/29/07 3:41 PM Page 3 2h EMERGENCIES • 3 A leash and loop restraint for an aggr...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-4.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-2-4.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-4.jpg' is RELEVANT.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-5.jpg\n",
      "DEBUG: Associated text context for model: 'A leash and loop restraint for an aggressive cat. The cat is immobilized by drawing the leash taut. ...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-5.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-2-5.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-5.jpg' is RELEVANT.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-3-6.jpg\n",
      "DEBUG: Associated text context for model: 'Aggressive cats can be picked up by slipping a leash or a loop of rope over the cat’s head and one f...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-3-6.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-3-6.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-3-6.jpg' is RELEVANT.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-7.jpg\n",
      "DEBUG: Associated text context for model: 'Some cats are quite cooperative while being held by the scruff of the neck. However, some cats will ...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-7.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-4-7.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-7.jpg' is RELEVANT.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-8.jpg\n",
      "DEBUG: Associated text context for model: '04_095300 ch01.qxp 10/29/07 3:41 PM Page 5 EMERGENCIES • 5 A cat bag restraint may be useful for tre...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-8.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-4-8.jpg': 'No.'\n",
      " ❌ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-8.jpg' is NOT relevant to its context. Skipping summarization.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-9.jpg\n",
      "DEBUG: Associated text context for model: 'EMERGENCIES • 5 A cat bag restraint may be useful for treating the head, but some cats really hate g...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-9.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-4-9.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-9.jpg' is RELEVANT.\n",
      "\n",
      "DEBUG: Processing image relevance for: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-5-10.jpg\n",
      "DEBUG: Associated text context for model: 'Uncooperative cats can be handled in several ways, depending on the degree of agitation. If the cat ...'\n",
      "WARNING: Image file not found: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-5-10.jpg. Cannot pass image data to model.\n",
      "DEBUG: LLM Raw Response for 'figure-5-10.jpg': 'Yes.'\n",
      "✅ Image '/Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-5-10.jpg' is RELEVANT.\n",
      "\n",
      "Number of relevant images for summarization: 9\n"
     ]
    }
   ],
   "source": [
    "import ollama # Ensure this is imported\n",
    "import os     # Ensure this is imported\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "relevant_images_to_summarize = []\n",
    "\n",
    "print(\"Checking image relevance with local textual context (now using Ollama's direct image passing)...\")\n",
    "for image_element in images_raw:\n",
    "    image_filename = image_element.text # This is the image path/filename\n",
    "    image_context = image_element.context # This is the locally associated context\n",
    "\n",
    "    print(f\"\\nDEBUG: Processing image relevance for: {image_filename}\")\n",
    "    print(f\"DEBUG: Associated text context for model: '{image_context[:100]}...'\")\n",
    "\n",
    "    if not image_context:\n",
    "        image_context = \"No specific text context was captured for this image, infer relevance from filename.\"\n",
    "\n",
    "    # Construct the multimodal messages list for ollama.chat\n",
    "    # Ollama library allows passing file paths directly in the 'images' list\n",
    "    messages_for_ollama = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Local Textual Context: {image_context}\\n\\nImage Filename: {os.path.basename(image_filename)}\\n\\nIs this image relevant to the content? Respond with 'yes' if relevant, 'no' if not. Only respond with 'yes' or 'no'.\",\n",
    "            \"images\": [] # Initialize empty list, add image_filename if exists\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if os.path.exists(image_filename):\n",
    "        messages_for_ollama[0][\"images\"].append(image_filename)\n",
    "        print(f\"DEBUG: Image file path '{os.path.basename(image_filename)}' IS included in the messages.\")\n",
    "    else:\n",
    "        print(f\"WARNING: Image file not found: {image_filename}. Cannot pass image data to model.\")\n",
    "\n",
    "    # Now, invoke ollama.chat directly\n",
    "    response_content = \"error\" # Default in case of invocation failure\n",
    "    try:\n",
    "        response_obj = ollama.chat(\n",
    "            model=\"minicpm-v:8b\",\n",
    "            messages=messages_for_ollama,\n",
    "            options={\"temperature\": 0.0} \n",
    "        )\n",
    "        response_content = response_obj['message']['content']\n",
    "        print(f\"DEBUG: LLM Raw Response for '{os.path.basename(image_filename)}': '{response_content.strip()}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to invoke Ollama for image relevance check on {image_filename}: {e}\")\n",
    "        # If there's an error, assume not relevant to avoid proceeding with bad data\n",
    "        response_content = \"no\"\n",
    "\n",
    "    # Check the response for relevance (case-insensitive and stripping whitespace)\n",
    "    if \"yes\" in response_content.lower().strip():\n",
    "        print(f\"✅ Image '{image_filename}' is RELEVANT.\")\n",
    "        relevant_images_to_summarize.append(image_element)\n",
    "    else:\n",
    "        print(f\" ❌ Image '{image_filename}' is NOT relevant to its context. Skipping summarization.\")\n",
    "\n",
    "print(f\"\\nNumber of relevant images for summarization: {len(relevant_images_to_summarize)}\")\n",
    "\n",
    "# Now, you would proceed with batch summarization using 'relevant_images_to_summarize'\n",
    "# instead of the full 'images_raw' list.\n",
    "# For example:\n",
    "# image_summaries = summarize_chain.batch([{'element': img} for img in relevant_images_to_summarize])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Remove Irrelevant Images From The Folder</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Deleting irrelevant images ---\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-1-1.jpg\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-1-2.jpg\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-3.jpg\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-4.jpg\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-2-5.jpg\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-3-6.jpg\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-7.jpg\n",
      "❓ Skipping deletion: Image file not found at /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-8.jpg\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-4-9.jpg\n",
      "Keeping relevant image: /Users/mas/Desktop/LLM_Veterinary_AI/figures/figure-5-10.jpg\n",
      "--- 🏁 Finished deleting images. Total deleted: 0 ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Assuming 'images_raw' contains all original image elements and\n",
    "# 'relevant_images_to_summarize' contains only the relevant ones from previous steps.\n",
    "\n",
    "# Get a set of relevant image paths for efficient lookup\n",
    "relevant_image_paths = {img_elem.text for img_elem in relevant_images_to_summarize}\n",
    "\n",
    "print(\"\\n--- Deleting irrelevant images ---\")\n",
    "images_deleted_count = 0\n",
    "for image_element in images_raw:\n",
    "    image_path = image_element.text\n",
    "    if image_path not in relevant_image_paths:\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                os.remove(image_path)\n",
    "                print(f\"✅ Successfully deleted irrelevant image: {image_path}\")\n",
    "                images_deleted_count += 1\n",
    "            except OSError as e:\n",
    "                print(f\" ❌ Error deleting file {image_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"❓ Skipping deletion: Image file not found at {image_path}\")\n",
    "    else:\n",
    "        print(f\"Keeping relevant image: {image_path}\")\n",
    "\n",
    "print(f\"--- 🏁 Finished deleting images. Total deleted: {images_deleted_count} ---\")\n",
    "\n",
    "# After deletion, you might want to update images_raw to only contain relevant ones\n",
    "# if you plan to reuse it later in the notebook, though `relevant_images_to_summarize`\n",
    "# already holds what you need for subsequent steps like summarization.\n",
    "# For example:\n",
    "# images_raw = relevant_images_to_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "import base64\n",
    "\n",
    "# Directories\n",
    "image_directory = \"./figures/shortExample2/\"\n",
    "output_directory = \"./figures/shortExample2/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Supported image formats\n",
    "supported_extensions = ('.png', '.jpg', '.jpeg')\n",
    "\n",
    "for filename in os.listdir(image_directory):\n",
    "    if filename.lower().endswith(supported_extensions):\n",
    "        image_path = os.path.join(image_directory, filename)\n",
    "        output_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "        output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "        print(f\"⏳ Working on summary for {filename}\")\n",
    "\n",
    "        # Check if the summary file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"☑️ Summary for {filename} already exists at {output_path}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Read and encode image in base64\n",
    "            with open(image_path, 'rb') as f:\n",
    "                image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "\n",
    "            # Send image to ollama for vision model processing\n",
    "            response = ollama.chat(\n",
    "                model='llava:7b',\n",
    "                messages=[\n",
    "                    {\n",
    "                        'role': 'user',\n",
    "                        'content': 'Describe the image in detail, focusing on any actions, techniques, or procedures depicted related to pet handling or care. Explain the purpose or context of the actions shown, if clear. Be concise and relevant to veterinary advice. If you think the images has nothing to do with veterinary, do not do anything.',  \n",
    "                        'images': [image_data]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Extract and save the generated summary\n",
    "            summary = response['message']['content']\n",
    "            \n",
    "            with open(output_path, 'w') as f:\n",
    "                f.write(summary)\n",
    "\n",
    "            print(f\"✅ Summary for {filename} saved to {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {filename}: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read image and summaries from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./figures/shortExample2/figure-15-14.jpg',\n",
       " './figures/shortExample2/figure-20-20.jpg',\n",
       " './figures/shortExample2/figure-14-12.jpg',\n",
       " './figures/shortExample2/figure-19-18.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "path =  \"./figures/shortExample2/\"\n",
    "# Get all .txt files in the directory\n",
    "file_paths = glob.glob(os.path.expanduser(os.path.join(path, \"*.txt\")))\n",
    "\n",
    "# Supported image formats\n",
    "supported_extensions = ('.png', '.jpg', '.jpeg')\n",
    "\n",
    "# Read each file and store its content in a list, and collect corresponding image paths\n",
    "img_summaries = []\n",
    "image_paths = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        img_summaries.append(file.read())\n",
    "    \n",
    "    # Derive the original image path from the summary file path\n",
    "    base_filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    found_image_path = None\n",
    "    for ext in supported_extensions:\n",
    "        potential_image_path = os.path.join(path, base_filename + ext)\n",
    "        if os.path.exists(potential_image_path):\n",
    "            found_image_path = potential_image_path\n",
    "            break\n",
    "    image_paths.append(found_image_path)\n",
    "\n",
    "# Clean up residual logging\n",
    "# cleaned_img_summary = [\n",
    "#     s.split(\"clip_model_load: total allocated memory: 201.27 MB\\\\n\\\\n\", 1)[1].strip() #Llava Model could left this message at every summary. This line is here to remove the overhead.\n",
    "#     for s in img_summaries\n",
    "# ]\n",
    "\n",
    "# Filter out entries where no corresponding image was found\n",
    "# This ensures cleaned_img_summary and image_paths remain aligned\n",
    "# filtered_img_summaries = []\n",
    "# filtered_image_paths = []\n",
    "# for i, summary in enumerate(cleaned_img_summary):\n",
    "#     if image_paths[i] is not None:\n",
    "#         filtered_img_summaries.append(summary)\n",
    "#         filtered_image_paths.append(image_paths[i])\n",
    "\n",
    "cleaned_img_summary = img_summaries\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, storing all those in a vector DB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"summaries\", embedding_function=GPT4AllEmbeddings()\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()  # <- Can we extend this to images\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma collection 'summaries' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have initialized your vectorstore variable (e.g., by running previous cells)\n",
    "# This will delete the Chroma collection named \"summaries\"\n",
    "vectorstore.delete_collection()\n",
    "print(\"Chroma collection 'summaries' has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add texts\n",
    "if texts:\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "    summary_texts = [\n",
    "        Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "        for i, s in enumerate(text_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_texts)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "\n",
    "# Add tables\n",
    "if tables:\n",
    "    table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "    summary_tables = [\n",
    "        Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "        for i, s in enumerate(table_summaries)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_tables)\n",
    "    retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "    # Add images\n",
    "# Add images\n",
    "if cleaned_img_summary:\n",
    "    img_ids = [str(uuid.uuid4()) for _ in cleaned_img_summary]\n",
    "    summary_img = [\n",
    "        Document(page_content=s, metadata={id_key: img_ids[i]})\n",
    "        for i, s in enumerate(cleaned_img_summary)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_img)\n",
    "    # Store the image path as the raw document for retrieval\n",
    "    retriever.docstore.mset(\n",
    "        list(zip(img_ids, image_paths))\n",
    "    )\n",
    " # Store the image summary as the raw document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./figures/shortExample2/figure-14-12.jpg',\n",
       " './figures/shortExample2/figure-15-14.jpg',\n",
       " './figures/shortExample2/figure-20-20.jpg',\n",
       " './figures/shortExample2/figure-19-18.jpg']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try_result = retriever.invoke(\"Images / figures with cat in a white background\")\n",
    "try_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from PIL import Image\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts from a list of documents/strings.\n",
    "    Handles both Document objects and direct image path strings.\n",
    "    Only the first identified image will be base64 encoded and marked for Markdown rendering.\n",
    "    Subsequent image paths will be added as plain text references.\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    image_processed = False # Flag to ensure only one image is processed visually\n",
    "\n",
    "    for doc in docs:\n",
    "        # Determine the content based on whether 'doc' is a Document object or a string\n",
    "        doc_content = doc.page_content if isinstance(doc, Document) else str(doc)\n",
    "\n",
    "        # Check if the content is an image path and if the file exists\n",
    "        if doc_content.lower().endswith(('.png', '.jpg', '.jpeg')) and os.path.exists(doc_content):\n",
    "            if not image_processed: # Process only the first image visually\n",
    "                image_path = doc_content\n",
    "                try:\n",
    "                    with open(image_path, 'rb') as f:\n",
    "                        image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "                        resized_image_data = resize_base64_image(image_data, size=(1300, 600))\n",
    "                        b64_images.append(resized_image_data)\n",
    "                    # Add the image path for Markdown rendering in the LLM response\n",
    "                    texts.append(f\"IMAGE_PATH_FOR_MD: {image_path}\")\n",
    "                    image_processed = True # Set flag to true after processing the first image\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {image_path}: {e}\")\n",
    "                    texts.append(doc_content) # If error, treat as text\n",
    "            else:\n",
    "                # For subsequent image paths, just add a textual reference without the special tag\n",
    "                texts.append(f\"An additional image related to the context was found at: {os.path.basename(doc_content)}\")\n",
    "        else:\n",
    "            # It's text or a table summary\n",
    "            texts.append(doc_content)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Construct the messages for the multimodal LLM.\n",
    "    `data_dict` will contain keys: 'context' (which is {'images': [...], 'texts': [...]}) and 'question'.\n",
    "    Instructs the LLM to render ALL image paths found via Markdown, striving to include images where relevant.\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Debugging: Print images being passed to the model\n",
    "    print(f\"DEBUG: Images being passed to LLM (visually): {len(data_dict['context']['images'])} image(s)\")\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        print(f\"DEBUG: First visual image (base64 snippet): {data_dict['context']['images'][0][:50]}...\")\n",
    "\n",
    "    # Adding image(s) to the messages if present (this is the visual input)\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis, with stronger instructions for Markdown rendering\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a veterinary assistant tasked with providing veterinary advice. \"\n",
    "            \"You will be given a mixed of text, tables, and image references. \"\n",
    "            \"Your primary goal is to use all provided information, including images, to answer the user's question comprehensively. \"\n",
    "            \"It is ESSENTIAL that you identify ALL instances of `IMAGE_PATH_FOR_MD: /path/to/image.jpg` in the context \"\n",
    "            \"and convert them directly into Markdown image syntax within your response. \"\n",
    "            \"For each image, provide a brief, accurate alt text description like `![Description of image content](/path/to/image.jpg)`. \"\n",
    "            \"For example, if the context contains `IMAGE_PATH_FOR_MD: ./figures/cat_pickup.jpg`, you MUST output `![Illustration of cat pickup technique](./figures/cat_pickup.jpg)`. \"\n",
    "            \"Include these Markdown images strategically where they best illustrate your points in the answer. \"\n",
    "            \"Do NOT omit any `IMAGE_PATH_FOR_MD:` entries; they must be rendered as Markdown images. \"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOllama(model=\"llama3.2-vision\")\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func) # This consumes the {\"context\": {\"images\": ..., \"texts\": ...}, \"question\": ...} dict\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "\n",
    "# Create RAG chain\n",
    "# Assuming 'retriever' is already defined from previous cells\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check retrieval\n",
    "query = \"What kind of cat need more calories per day?\"\n",
    "docs = retriever.invoke(query, limit=8)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 4 documents:\n",
      "\n",
      "--- Document 1 ---\n",
      "Text Content:\n",
      "Counting Calories\n",
      "\n",
      "Unless maintaining a good body weight is a problem, senior cats should be on a reduced-calorie diet. In general, an older cat who is neither too fat nor too thin needs about 20 calories per pound (.45 kg) of body weight per day—and sometimes even less—to meet her caloric needs. These are guidelines, and the exact amount needed to keep your cat at an ideal weight may vary. Various health conditions may also dictate that your cat needs more or fewer calories.\n",
      "\n",
      "--- Document 2 ---\n",
      "Text Content:\n",
      "1 The values for amount per kilogram of dry matter have been calculated assuming a dietary energy density of 4,000 calories ME per kilogram of food. If the energy density of the diet is not 4,000 calo- ries ME per kilogram, then to calculate the per kilogram of dry matter for each nutrient, multiply the value for the nutrient by the energy density of the pet food (in calories ME per kilogram) and divide by 4,000.\n",
      "\n",
      "2 0.02 g arginine should be added for every gram of crude protein above 200 g for the Recommended\n",
      "\n",
      "--- Document 3 ---\n",
      "Text Content:\n",
      "FEEDING ADULT CATS\n",
      "\n",
      "The actual amount of food a cat needs varies among cats of equal weight because of differences in metabolic rate and activity level. Labels only provide guidelines—the actual amount to feed must be customized to the individual cat. Spayed and neutered cats have a much lower metabolism than intact cats.\n",
      "\n",
      "Generally, an active adult cat will need about 30 to 35 calories per pound (.45 kg) of body weight per day, and some will do well with about 25 calories per pound per day. An inactive cat will need about 18 calories per pound (.45 kg) of body weight per day. Even if they are active, many spayed and neutered cats do very well on the lower calorie estimate.\n",
      "\n",
      "Pregnant and nursing cats have much higher requirements—figure about 45 calories per pound of body weight per day during the last trimester of preg- nancy and as high as 140 calories per pound during the peak of lactation.\n",
      "\n",
      "—o—\n",
      "\n",
      "21_095300 ch18.qxp 10/29/07 3:49 PM Page 504\n",
      "\n",
      "--- Document 4 ---\n",
      "Text Content:\n",
      "491\n",
      "\n",
      "—o—\n",
      "\n",
      "18\n",
      "\n",
      "21_095300 ch18.qxp 10/29/07 3:49 PM Page 492\n",
      "\n",
      "‘7 7\n",
      "\n",
      "492 • CAT OWNER’S HOME VETERINARY HANDBOOK\n",
      "\n",
      "used for energy but also for a healthy nervous system, skin, and many meta- bolic processes. Ideally, a cat’s diet should be at least 9 percent fat.\n",
      "\n",
      "Cats lack many of the amylases, which are enzymes that aid in carbohy- drate digestion. Large amounts of carbohydrates may decrease the efficiency of protein digestion as well as cause high levels of blood glucose.\n",
      "\n",
      "--- LLM Response ---\n",
      "DEBUG: Images being passed to LLM (visually): 0 image(s)\n",
      "The type of cat that needs more calories per day is typically an active or pregnant/nursing cat. Here's a breakdown of the different calorie needs for cats:\n",
      "\n",
      "**Active Cats**\n",
      "An active adult cat will need about 30 to 35 calories per pound (.45 kg) of body weight per day, and some will do well with about 25 calories per pound per day. An illustration of an active cat playing with a ball of yarn is shown: ![Active cat playing with yarn](IMAGE_PATH_FOR_MD: ./figures/active_cat_yarn.jpg)\n",
      "\n",
      "**Pregnant and Nursing Cats**\n",
      "Pregnant and nursing cats have much higher requirements—figure about 45 calories per pound of body weight per day during the last trimester of pregnancy and as high as 140 calories per pound during the peak of lactation. An image of a pregnant cat with her kittens is shown: ![Pregnant cat with kittens](IMAGE_PATH_FOR_MD: ./figures/pregnant_cat_kittens.jpg)\n",
      "\n",
      "**Spayed/Neutered Cats**\n",
      "Spayed and neutered cats have a much lower metabolism than intact cats. Even if they are active, many spayed and neutered cats do very well on the lower calorie estimate. A picture of a spayed cat is shown: ![Spayed cat](IMAGE_PATH_FOR_MD: ./figures/spayed_cat.jpg)\n",
      "\n",
      "**Senior Cats**\n",
      "Unless maintaining a good body weight is a problem, senior cats should be on a reduced-calorie diet. In general, an older cat who is neither too fat nor too thin needs about 20 calories per pound (.45 kg) of body weight per day—and sometimes even less—to meet her caloric needs. An image of a senior cat is shown: ![Senior cat](IMAGE_PATH_FOR_MD: ./figures/senior_cat.jpg)\n",
      "\n",
      "In summary, cats that need more calories per day are typically active or pregnant/nursing cats. Spayed/Neutered cats may also require more calories, but the exact amount will depend on their individual metabolism and activity level. Senior cats, on the other hand, may require fewer calories to maintain a healthy weight.\n"
     ]
    }
   ],
   "source": [
    "# Your query\n",
    "query = \"What kind of chat need more calories per day? Include images\"\n",
    "\n",
    "# Retrieve documents\n",
    "docs = retriever.invoke(query, limit=6)\n",
    "\n",
    "print(f\"Retrieved {len(docs)} documents:\")\n",
    "\n",
    "# Iterate and display each document\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_content = doc.page_content if isinstance(doc, Document) else str(doc)\n",
    "\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    if doc_content.lower().endswith(('.png', '.jpg', '.jpeg')) and os.path.exists(doc_content):\n",
    "        # It's an image path\n",
    "        image_path = doc_content\n",
    "        try:\n",
    "            with open(image_path, 'rb') as f:\n",
    "                image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "                # Resize for display if needed, using your existing function\n",
    "                # You might want to use a smaller size here for better display in notebook\n",
    "                resized_image_data = resize_base64_image(image_data, size=(600, 300))\n",
    "                plt_img_base64(resized_image_data)\n",
    "                print(f\"Displayed image from: {image_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading or displaying image {image_path}: {e}\")\n",
    "    else:\n",
    "        # It's text or a table summary\n",
    "        print(f\"Text Content:\\n{doc_content}\")\n",
    "\n",
    "print(\"\\n--- LLM Response ---\")\n",
    "# Finally, get the LLM's answer using the chain\n",
    "llm_response = chain_multimodal_rag.invoke(query)\n",
    "print(llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The type of cat that needs more calories per day is typically an active or pregnant/nursing cat. Here's a breakdown of the different calorie needs for cats:\n",
       "\n",
       "**Active Cats**\n",
       "An active adult cat will need about 30 to 35 calories per pound (.45 kg) of body weight per day, and some will do well with about 25 calories per pound per day. An illustration of an active cat playing with a ball of yarn is shown: ![Active cat playing with yarn](IMAGE_PATH_FOR_MD: ./figures/active_cat_yarn.jpg)\n",
       "\n",
       "**Pregnant and Nursing Cats**\n",
       "Pregnant and nursing cats have much higher requirements—figure about 45 calories per pound of body weight per day during the last trimester of pregnancy and as high as 140 calories per pound during the peak of lactation. An image of a pregnant cat with her kittens is shown: ![Pregnant cat with kittens](IMAGE_PATH_FOR_MD: ./figures/pregnant_cat_kittens.jpg)\n",
       "\n",
       "**Spayed/Neutered Cats**\n",
       "Spayed and neutered cats have a much lower metabolism than intact cats. Even if they are active, many spayed and neutered cats do very well on the lower calorie estimate. A picture of a spayed cat is shown: ![Spayed cat](IMAGE_PATH_FOR_MD: ./figures/spayed_cat.jpg)\n",
       "\n",
       "**Senior Cats**\n",
       "Unless maintaining a good body weight is a problem, senior cats should be on a reduced-calorie diet. In general, an older cat who is neither too fat nor too thin needs about 20 calories per pound (.45 kg) of body weight per day—and sometimes even less—to meet her caloric needs. An image of a senior cat is shown: ![Senior cat](IMAGE_PATH_FOR_MD: ./figures/senior_cat.jpg)\n",
       "\n",
       "In summary, cats that need more calories per day are typically active or pregnant/nursing cats. Spayed/Neutered cats may also require more calories, but the exact amount will depend on their individual metabolism and activity level. Senior cats, on the other hand, may require fewer calories to maintain a healthy weight."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Assuming llm_response contains the string output from your chain\n",
    "# llm_response = chain_multimodal_rag.invoke(query) # Run this to get the response\n",
    "display(Markdown(llm_response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
